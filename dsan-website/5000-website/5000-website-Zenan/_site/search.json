[
  {
    "objectID": "aboutyou.html#backgrounds",
    "href": "aboutyou.html#backgrounds",
    "title": "About Me",
    "section": "Backgrounds:",
    "text": "Backgrounds:\nHey everyone, I am Zenan Wang, commonly known as Will, hailing from Nanjing, China. I completed my undergraduate studies at UNC Chapel Hill, achieving a double major in Mathematics and Statistics. My interests include cycling, fitness activities, and indulging in video games for leisure.\n\nLinkedin link: https://www.linkedin.com/in/zenan-wang-062695260/"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a method used in machine learning and data analysis to group similar data points together based on certain features or characteristics. The goal of clustering is to partition a dataset into subsets, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. This helps in identifying patterns, structures, or hidden relationships within the data.\n\n\n\nIn this analysis, the feature data X pertains to the state the accident took place, while the Y variables include ‘CP,’ ‘tsla+cp,’ ‘VTAD,’ ‘Claimed,’ ‘Tesla_occupant,’ and ‘Other_vehicle.’ Notably, conventional visualization techniques like PCA and t-SNE prove ineffective due to the absence of a distinct separation between clusters. Consequently, clustering methods such as K-means are employed in this section for a more suitable exploration of the data’s inherent patterns.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n\n\n\n\n\ndf = pd.read_csv('./cleandata/cleanTelsa.csv')\nfeatures = df[['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle']]\ntarget = df['State']\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    km = KMeans(n_clusters=k, random_state=8)\n    km.fit(features_scaled)\n    inertias.append(km.inertia_)\n    \nplt.plot(ks, inertias, marker='o')\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\np_comps = pca.fit_transform(features_scaled)\np_comp1 = p_comps[:,0]\np_comp2 = p_comps[:,1]\nkm = KMeans(n_clusters=3,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x16069e65870&gt;\n\n\n\n\n\n\n\n\nIt appears that there isn’t a substantial difference between utilizing k=3 and k=4. Therefore, for the sake of diversity, we will adhere to using k=4.\n\nkm = KMeans(n_clusters=4,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x1606a8edf90&gt;\n\n\n\n\n\n\n\n\nA logical progression in our analysis involves examining the distinctions among the three clusters concerning the three features employed for clustering. Instead of relying on scaled features, we revert to using the unscaled features to facilitate a more interpretable exploration of the differences.\n\n\n\n\nimport seaborn as sns\ndf['cluster'] = km.labels_\nmelt_car = pd.melt(df,id_vars='cluster',var_name=\"predictor\",value_name=\"percent\",\n                 value_vars=['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle'] )\nsns.violinplot(data=melt_car,y='predictor',x='percent',hue='cluster')\n\n&lt;Axes: xlabel='percent', ylabel='predictor'&gt;\n\n\n\n\n\n\n\n\nFrom the presented plot, it is now evident that distinct clusters of states may necessitate varied interventions, such as implementing safety measures like road cushioning or enforcing more stringent traffic laws tailored to each group’s specific characteristics."
  },
  {
    "objectID": "clustering.html#clustering-on-the-dataset-tesla-deaths---deaths",
    "href": "clustering.html#clustering-on-the-dataset-tesla-deaths---deaths",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a method used in machine learning and data analysis to group similar data points together based on certain features or characteristics. The goal of clustering is to partition a dataset into subsets, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. This helps in identifying patterns, structures, or hidden relationships within the data.\n\n\n\nIn this analysis, the feature data X pertains to the state the accident took place, while the Y variables include ‘CP,’ ‘tsla+cp,’ ‘VTAD,’ ‘Claimed,’ ‘Tesla_occupant,’ and ‘Other_vehicle.’ Notably, conventional visualization techniques like PCA and t-SNE prove ineffective due to the absence of a distinct separation between clusters. Consequently, clustering methods such as K-means are employed in this section for a more suitable exploration of the data’s inherent patterns.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n\n\n\n\n\ndf = pd.read_csv('./cleandata/cleanTelsa.csv')\nfeatures = df[['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle']]\ntarget = df['State']\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    km = KMeans(n_clusters=k, random_state=8)\n    km.fit(features_scaled)\n    inertias.append(km.inertia_)\n    \nplt.plot(ks, inertias, marker='o')\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\np_comps = pca.fit_transform(features_scaled)\np_comp1 = p_comps[:,0]\np_comp2 = p_comps[:,1]\nkm = KMeans(n_clusters=3,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x16069e65870&gt;\n\n\n\n\n\n\n\n\nIt appears that there isn’t a substantial difference between utilizing k=3 and k=4. Therefore, for the sake of diversity, we will adhere to using k=4.\n\nkm = KMeans(n_clusters=4,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x1606a8edf90&gt;\n\n\n\n\n\n\n\n\nA logical progression in our analysis involves examining the distinctions among the three clusters concerning the three features employed for clustering. Instead of relying on scaled features, we revert to using the unscaled features to facilitate a more interpretable exploration of the differences.\n\n\n\n\nimport seaborn as sns\ndf['cluster'] = km.labels_\nmelt_car = pd.melt(df,id_vars='cluster',var_name=\"predictor\",value_name=\"percent\",\n                 value_vars=['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle'] )\nsns.violinplot(data=melt_car,y='predictor',x='percent',hue='cluster')\n\n&lt;Axes: xlabel='percent', ylabel='predictor'&gt;\n\n\n\n\n\n\n\n\nFrom the presented plot, it is now evident that distinct clusters of states may necessitate varied interventions, such as implementing safety measures like road cushioning or enforcing more stringent traffic laws tailored to each group’s specific characteristics."
  },
  {
    "objectID": "clustering.html#clustering-on-the-dataset-traffic-accidents-and-vehicles-gas-car",
    "href": "clustering.html#clustering-on-the-dataset-traffic-accidents-and-vehicles-gas-car",
    "title": "Clustering",
    "section": "Clustering on the dataset: Traffic Accidents and Vehicles (gas car)",
    "text": "Clustering on the dataset: Traffic Accidents and Vehicles (gas car)\n\n\nCode\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nX = data.drop(columns='Accident_Severity')\ny = data['Accident_Severity']\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    # In the dataset we only have float and int64.\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    # Categorical\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\ndata = X[numerical_features]\nSS=StandardScaler()\nX=pd.DataFrame(SS.fit_transform(data), columns=data.columns)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nprincipal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\nkmeans = KMeans(n_clusters=5, n_init=15, max_iter=500, random_state=0)\nclusters = kmeans.fit_predict(X)\ncentroids = kmeans.cluster_centers_\ncentroids_pca = pca.transform(centroids)\n\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\base.py:464: UserWarning:\n\nX does not have valid feature names, but PCA was fitted with feature names\n\n\n\n\nPlot the clustering results both in 2-D and 3-D plot\n\nplt.figure(figsize=(8,6))\nplt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], c=clusters, cmap=\"brg\", s=40)\nplt.scatter(x=centroids_pca[:,0], y=centroids_pca[:,1], marker=\"x\", s=100, linewidths=3, color=\"black\")\nplt.title('PCA plot in 2D')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\nText(0, 0.5, 'PC2')\n\n\n\n\n\n\npca = PCA(n_components=3)\ncomponents = pca.fit_transform(X)\nimport plotly.express as px\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=clusters, size=0.1*np.ones(len(X)), opacity = 1,\n    title='PCA plot in 3D',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    width=650, height=500\n)\nfig.show()\n\n\n                                                \n\n\n\n\nIn summary:\n\nClustering is not distinctly visible in both 2-D and 3-D plots, primarily due to the extensive size of the dataset, leading to overlapping scatter plots. However, subsetting the dataset would improve visualization clarity.\nUtilizing clustering methods enables the categorization of the entire dataset into smaller groups, each representing distinct types of accidents. Subsequently, any of these groups can be selected for in-depth analysis."
  },
  {
    "objectID": "dataexplore.html",
    "href": "dataexplore.html",
    "title": "Data exploring",
    "section": "",
    "text": "EDA, or Exploratory Data Analysis, is a crucial phase in the data analysis process that involves summarizing the main characteristics of a dataset, often with the help of statistical graphics and other data visualization methods. The primary goal of EDA is to understand the structure and key features of the data, discover patterns, identify potential outliers, and generate hypotheses for further analysis.\n\n\n\nTesla Deaths - Deaths\nUK Road Safety: Traffic Accidents and Vehicles (gas car)"
  },
  {
    "objectID": "dataexplore.html#introduction",
    "href": "dataexplore.html#introduction",
    "title": "Data exploring",
    "section": "",
    "text": "EDA, or Exploratory Data Analysis, is a crucial phase in the data analysis process that involves summarizing the main characteristics of a dataset, often with the help of statistical graphics and other data visualization methods. The primary goal of EDA is to understand the structure and key features of the data, discover patterns, identify potential outliers, and generate hypotheses for further analysis.\n\n\n\nTesla Deaths - Deaths\nUK Road Safety: Traffic Accidents and Vehicles (gas car)"
  },
  {
    "objectID": "dataexplore.html#first-dataset-tesla-deaths---deaths",
    "href": "dataexplore.html#first-dataset-tesla-deaths---deaths",
    "title": "Data exploring",
    "section": "First dataset: Tesla Deaths - Deaths",
    "text": "First dataset: Tesla Deaths - Deaths\n\nPre-process the data and take a look at the cleaned data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\ndf = pd.read_csv(\"./Data/Tesla Deaths - Deaths.csv\")\nnew_df = pd.DataFrame()\nfor i in range(len(df.columns[:14]),1,-1):\n    new_df.insert(0,df.columns[i],df[df.columns[i]])\ndf = new_df\nfor i in range(5,10):\n    df[df.columns[i]] = df[df.columns[i]].fillna(\"-\")\n    \nfor i in range(11,13):\n    df[df.columns[i]] = df[df.columns[i]].fillna('-')\ndf = df.dropna()\ndf.columns = ['Date','Country','State','Description','Deaths',\"Tesla_driver\",\"Tesla_occupant\",\"Other_vehicle\",\"CP\",\"tsla+cp\",\"Model\",\"Claimed\",\"VTAD\"]\nfor i in range(5,13):\n    for b in range(len(df)):\n        if \"-\"in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 0\n        elif \"1\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 1\n        elif \"2\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 2\n        elif \"3\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 3\n        elif \"4\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 4\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n\n\nCode\ndf.head()\ndf.to_csv('./cleandata/cleanTelsa.csv', index=False)"
  },
  {
    "objectID": "dataexplore.html#start-eda",
    "href": "dataexplore.html#start-eda",
    "title": "Data exploring",
    "section": "Start EDA",
    "text": "Start EDA\n\nThe ‘Date’ variable is not in the default date format\n\n\nCode\ndf.loc[:, \"event_year\"] = 0\ndf.loc[:, \"event_month\"] = 0\ndf.loc[:, \"event_day\"] = 0\nfor i in range(len(df)):\n    df.loc[df.index[i], \"event_year\"] = int(df[\"Date\"].values[i].split('/')[2])\n    df.loc[df.index[i], \"event_month\"] = int(df[\"Date\"].values[i].split('/')[0])\n    df.loc[df.index[i], \"event_day\"] = int(df[\"Date\"].values[i].split('/')[1])\ndf['Date']\n\n\n0       1/17/2023\n1        1/7/2023\n2        1/7/2023\n3      12/22/2022\n4      12/19/2022\n          ...    \n289     7/14/2014\n290      7/4/2014\n291      7/4/2014\n292     11/2/2013\n293      4/2/2013\nName: Date, Length: 294, dtype: object\n\n\n\n\nLet’s see the distribution of crashes around the world\nIt seems that USA has the most accidents, then followed by China and Germany.\n\n\nCode\nx = df[\"Country\"].value_counts().index\ny = df[\"Country\"].value_counts().values\nplt.figure(figsize=(20,8))\nfor i in range(len(x)):\n    height = y[i]\n    plt.text(x[i], height + 0.25, '%.1f' %height, ha='center', va='bottom', size = 12)\nplt.bar(x,y,color='#e35f62')\n\n\n&lt;BarContainer object of 23 artists&gt;\n\n\n\n\n\n\n\nLet’s look at the number of accidents per month\n\n\nCode\nplt.figure(figsize=(20,8))\nx = df[\"event_month\"].value_counts().sort_index().index\ny = df[\"event_month\"].value_counts().sort_index().values\nfor i in range(len(x)):\n    height = y[i]\n    plt.text(x[i], height + 0.5, '%.1f' %height, ha='center', va='bottom', size = 12)\nplt.title(\"Number of accidents per month in the total year\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Number of events\")\nplt.bar(x,y)\n\n\n&lt;BarContainer object of 12 artists&gt;\n\n\n\n\n\n\n\nIt appears that there are relatively more accidents in November and December.\n\n\nLet’s look at the distribution of the following variables:\n\n\nCode\nd_list = [\"Deaths\",\"Tesla_driver\",\"Tesla_occupant\",\"CP\",\"tsla+cp\",\"Other_vehicle\"]\nlabel = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\nplt.figure(figsize = (16,16))\nfor b in range(len(d_list)):\n    size = df[d_list[b]].value_counts().values\n    colors = []\n    label = df[d_list[b]].value_counts().index\n    plt.axis(\"equal\")\n    plt.rc(\"font\",family=\"Malgun Gothic\")\n    plt.rc('legend', fontsize=10)\n    plt.subplot(2,3,b+1)\n    plt.title(d_list[b]+\" - total:\" +str(len(df[d_list[b]])))\n    plt.pie(size,labels=label, autopct = \"%.1f%%\")\n    plt.legend()\n\n\n\n\n\n\n\nThe above collection of pie charts indicates that the majority of the accidents involve one casualty.\n\n\nwhy are there more accidents in November and December? Does that necessarily mean Tesla is maneuverable?\nSince there is a ‘description’ variable in the dataset, which briefly depicts the possible cause and the crash scene, I will classify the accidents into smaller categories.\n\n\nCode\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1]\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef lemmatize_text(text):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(text)]\ndata = df[\"Description\"]\n\nlemmatized_data = [lemmatize_text(text) for text in data]\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform([' '.join(text) for text in lemmatized_data])\n\nkmeans = KMeans(n_clusters= 5, random_state=0)\nkmeans.fit(X)\nclusters = kmeans.predict(X)\n\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\nLet’s see how the classification goes.\n\n\nCode\ndf[\"Description\"].value_counts()\n\n\nDescription\n Tesla kills pedestrian                         10\n Tesla kills motorcyclist                        6\n Tesla into oncoming traffic                     5\n Tesla rear ends stopped car                     4\n Tesla drives off cliff                          4\n                                                ..\n Tesla crashes into trees                        1\n Multi-crash involving DUI                       1\n Tesla loses control and drives into river       1\n Crash in public area including 20 injuries      1\n Tesla veers into opposite lane                  1\nName: count, Length: 248, dtype: int64\n\n\n\n\nLet’s look at the distribution of the classification\n\n\nCode\ndf.loc[:, \"cluster\"] = 0\nfor i, item in enumerate(df[\"Description\"]):\n    for cluster, description in zip(clusters, data):\n        if item == description:\n            df[\"cluster\"][i] = cluster\naccident_0 = []\naccident_1 = []\naccident_2 = []\naccident_3 = []\naccident_4 = []\nfor i, cluster in enumerate(clusters):\n    if cluster == 0:\n        accident_0.append(data[i])\n    elif cluster == 1:\n        accident_1.append(data[i])\n    elif cluster == 2:\n        accident_2.append(data[i])\n    elif cluster == 3:\n        accident_3.append(data[i])\n    elif cluster == 4:\n        accident_4.append(data[i])\naccident_0_score = []\naccident_1_score = []\naccident_2_score = []\naccident_3_score = []\naccident_4_score = []\n\ndef get_score_list(a,b1):\n    for i in range(len(a)):\n        for b in range(len(df[\"Description\"].value_counts().index)):\n            if df[\"Description\"].value_counts().index[b] == a[i]:\n                b1.append(df[\"Description\"].value_counts().values[b])\n                \nget_score_list(accident_0,accident_0_score)\nget_score_list(accident_1,accident_1_score)\nget_score_list(accident_2,accident_2_score)\nget_score_list(accident_3,accident_3_score)\nget_score_list(accident_4,accident_4_score)\nx = [0,1,2,3,4]\ny = [len(accident_0),len(accident_1),len(accident_2),len(accident_3),len(accident_4)]\nplt.figure(figsize=(20,10))\n# plt.title(\"accident type's counts\")\nplt.xlabel(\"accident type\")\nplt.ylabel(\"counts of accident type\")\nfor i in range(len(x)):\n    height = y[i]\n    plt.text(x[i], height + 0.25, '%.1f' %height, ha='center', va='bottom', size = 12)\nplt.bar(x,y,color='red')\n\n\n&lt;BarContainer object of 5 artists&gt;\n\n\n\n\n\n\n\nLet’s see what type-2 accident is about.\nIt seems that this type of collision has little to do with the maneuverability of Tesla.\n\n\nCode\ndf[df['cluster'] == 2].head(10)\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla_driver\nTesla_occupant\nOther_vehicle\nCP\ntsla+cp\nModel\nClaimed\nVTAD\nevent_year\nevent_month\nevent_day\ncluster\n\n\n\n\n8\n12/11/2022\nUSA\nMO\nCollision at intersection\n1.0\n0\n0\n1\n0\n0\n0\n0\n0\n2022\n12\n11\n2\n\n\n9\n12/6/2022\nCanada\n-\nTesla veers, collides with truck\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n12\n6\n2\n\n\n10\n11/28/2022\nChina\n-\nTesla runs red light, collides with two cars\n2.0\n0\n0\n2\n0\n0\nY\n0\n0\n2022\n11\n28\n2\n\n\n15\n11/12/2022\nUSA\nCA\nMulti-vehicle accident\n1.0\n0\n0\n1\n0\n0\n0\n0\n0\n2022\n11\n12\n2\n\n\n18\n11/4/2022\nUSA\nIL\nCollision at intersection, Tesla driver dies ...\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n11\n4\n2\n\n\n20\n10/18/2022\nUSA\nFL\nTesla collides with minivan, engulfed by flames\n4.0\n1\n1\n2\n0\n2\n0\n0\n0\n2022\n10\n18\n2\n\n\n26\n9/16/2022\nUSA\nGA\nTesla loses control and crashes into bus shel...\n1.0\n0\n0\n0\n1\n1\n0\n1\n0\n2022\n9\n16\n2\n\n\n27\n9/13/2022\nUSA\nCA\nTesla runs off highway\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n9\n13\n2\n\n\n28\n9/12/2022\nUSA\nNY\nTesla runs off road, catches fire\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n9\n12\n2\n\n\n29\n9/7/2022\nUSA\nCA\nMotorcycle collides with Tesla\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n9\n7\n2\n\n\n\n\n\n\n\n\n\nLet’s see what type-0 accident is about.\nIt seems that this type of collision also has little to do with the maneuverability of Tesla.\n\n\nCode\ndf[df['cluster'] == 0].head(10)\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla_driver\nTesla_occupant\nOther_vehicle\nCP\ntsla+cp\nModel\nClaimed\nVTAD\nevent_year\nevent_month\nevent_day\ncluster\n\n\n\n\n0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2023\n1\n17\n0\n\n\n1\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2023\n1\n7\n0\n\n\n3\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n12\n22\n0\n\n\n4\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n0\n0\n0\n1\n1\n0\n0\n0\n2022\n12\n19\n0\n\n\n7\n12/11/2022\nUSA\nCA\nTesla crashes into wall\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n12\n11\n0\n\n\n13\n11/18/2022\nChina\n-\nTesla crashes into dump truck\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n11\n18\n0\n\n\n19\n10/19/2022\nUSA\nCA\nMulticar crash\n1.0\n1\n0\n0\n0\n1\n1\n0\n0\n2022\n10\n19\n0\n\n\n21\n10/12/2022\nUK\n-\nTesla crashes into ditch\n1.0\n1\n0\n0\n0\n1\n2\n0\n0\n2022\n10\n12\n0\n\n\n24\n9/18/2022\nUSA\nSC\nTesla crashes into tree, ignites\n2.0\n1\n1\n0\n0\n2\n0\n0\n0\n2022\n9\n18\n0\n\n\n25\n9/18/2022\nUSA\nMD\nMulti-car accident\n1.0\n0\n0\n1\n0\n0\n0\n0\n0\n2022\n9\n18\n0\n\n\n\n\n\n\n\n\n\nNumber of deaths in each type of accident\n\n\nCode\nfor i in range(0,5):\n    df[df[\"cluster\"]==i][\"Deaths\"].value_counts().sort_index().plot(figsize=(20,8))\n    plt.xlabel(\"total Death\")\n    plt.ylabel(\"ac type Counts\")\n    plt.title(\"ac type Counts - Death\")\n    plt.legend(\"01234\")\n\n\n\n\n\n\n\nDistribution of each type of accident at each year recorded\n\n\nCode\ndef make_Graph(feature):\n    x_s = df[feature].value_counts().sort_index().index\n    y_s = df[feature].value_counts().sort_index().values\n    y0_s = df[df[\"cluster\"]==0][feature].value_counts().sort_index()\n    y1_s = df[df[\"cluster\"]==1][feature].value_counts().sort_index()\n    y2_s = df[df[\"cluster\"]==2][feature].value_counts().sort_index()\n    y3_s = df[df[\"cluster\"]==3][feature].value_counts().sort_index()\n    y4_s = df[df[\"cluster\"]==4][feature].value_counts().sort_index()\n    def fill_y(x,y):\n        for i in x:\n            if i not in y.index:\n                y[i] = 0\n    fill_y(x_s,y0_s)\n    fill_y(x_s,y1_s)\n    fill_y(x_s,y2_s)\n    fill_y(x_s,y3_s)\n    fill_y(x_s,y4_s)\n    \n#     plt.figure(figsize=(10,8))\n    ax,ax0,ax1,ax2,ax3,ax4 = plt.gca(),plt.gca(),plt.gca(),plt.gca(),plt.gca(),plt.gca()\n\n    for i in range(len(x_s)):\n        height = y_s[i]\n        plt.text(x_s[i], height + 0.15, '%.1f' %height, ha='center', va='bottom', size = 12)\n\n    ax.bar(x_s, y_s, color='orange', linestyle='--')\n    ax.set_ylabel(feature+' - Counts', fontsize=10)\n    ax.tick_params('y', colors='blue') \n\n    ax0.plot(x_s, y0_s.sort_index().values, color='yellow', linestyle='--',label = \"0\")\n    ax1.plot(x_s, y1_s.sort_index().values, color='blue', linestyle='--',label = \"1\")\n    ax2.plot(x_s, y2_s.sort_index().values, color='red', linestyle='--',label = \"2\")\n    ax3.plot(x_s, y3_s.sort_index().values, color='pink', linestyle='--',label = \"3\")\n    ax4.plot(x_s, y4_s.sort_index().values, color='green', linestyle='--',label = \"4\")\n\n    ax1.set_xlabel(feature, fontsize=10)\n    plt.title(feature+\" Accident counts & Accident Type counts\",fontsize=13)\n    plt.tight_layout()\n    plt.legend()\n    plt.figure(figsize=(20,8))\nmake_Graph(\"event_year\")\n\n\n\n\n\n&lt;Figure size 1920x768 with 0 Axes&gt;\n\n\n\n\nConclusion:\nWe do not have enough evidence to claim that the maneuverability of Tesla is flawed, since most accidents are caused by human errors. However, more accidents did occur at latest years, which might be due to the largely increased number of Tesla on road."
  },
  {
    "objectID": "dataexplore.html#second-dataset-uk-road-safety-traffic-accidents-and-vehicles-gas-car",
    "href": "dataexplore.html#second-dataset-uk-road-safety-traffic-accidents-and-vehicles-gas-car",
    "title": "Data exploring",
    "section": "Second dataset: UK Road Safety: Traffic Accidents and Vehicles (gas car)",
    "text": "Second dataset: UK Road Safety: Traffic Accidents and Vehicles (gas car)\n\n\nCode\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.pipeline import Pipeline\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import boxcox\nimport pickle\nfrom numpy.random import seed\nseed(862)\nfrom tensorflow.random import set_seed\nset_seed(862)\ndata = pd.read_csv(\"./Data/RoadAccident.csv\")\ndata.drop(['Accident_Index','Datetime'], axis = 1,inplace=True) \nd = {\"Feature\":[i for i in data.columns]    ,\"Nunique\" :data.nunique().values ,'Type' : data.dtypes.values, \"No: of nulls\" : data.isnull().sum() }\ndescription = pd.DataFrame(data = d)\ndescription\ndata[\"Season\"]=data[\"Season\"].astype(str)\ndata[\"Month_of_Year\"]=data[\"Month_of_Year\"].astype(str)\ndata[\"Day_of_Week\"]=data[\"Day_of_Week\"].astype(str)\ndata[\"Year\"]=data[\"Year\"].astype(str)\ndata[\"Number_of_Vehicles\"]=data[\"Number_of_Vehicles\"].astype(str)\nCounter(data['Accident_Severity'])\n\n\nCounter({'Slight': 56705, 'Fatal_Serious': 18845})\n\n\n\nPlot count plots for the categorical data and histogram for numerical data\n\n\nCode\ncat_data  = data.select_dtypes(exclude=[np.number])\nfor i in cat_data:  \n  plt.figure(figsize=(10,10))# Creating an empty plot \n  ax=sns.countplot(x=cat_data[i],hue=data[\"Accident_Severity\"])# Countplot of airlines\n  plt.tick_params(labelsize=10)# changing the label sizes  \n  plt.ylabel(\"Count\" ,fontsize=10) #Adding y-label\n  #plt.title(\"\\n\", cat_data.columns.values,\"\\n\",fontsize=25) # Adding plot title\n  for p in ax.patches:\n      ax.annotate('{}'.format(p.get_height()),(p.get_x()+0.25,p.get_height()+5)) # Adding the count above the bars\n  plt.show() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at the distribution of all numerical variables\n\n\nCode\ndef diagnostic_plots(df, variable):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    df[variable].hist()\n    print(str(variable))\n    plt.subplot(1, 2, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.show()\nnum_data  = data.select_dtypes(include=[np.number])\nfor i in num_data:\n    diagnostic_plots(num_data, i) \n\n\nLatitude\nLongitude\nDriver_IMD_Decile\nSpeed_limit\nDay_of_Month\nHour_of_Day\nAge_of_Driver\nAge_of_Vehicle\nEngine_CC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of the above distribution plots:\n\nFor some categorical variable, there are many ranks that have very low frequency. For example, only the ‘fine’ rank of the ‘Weather’ variable has significant count, hence the other ranks might be grouped into one for easy processing.\nFor some numerical variables, some features are not normally distributed. This would be taken care of in future analysis.\n\n\n\nLook at the heatmap of the numerical variables\n\n\nCode\ncorrmat = data.select_dtypes(include=[np.number]).corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(14,6))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n\n\n\n\n\n\n\nThe above heatmap does not indicate any unusual correlation."
  },
  {
    "objectID": "dataexplore.html#tools-and-software",
    "href": "dataexplore.html#tools-and-software",
    "title": "Data exploring",
    "section": "Tools and software",
    "text": "Tools and software\nThe creation of this tab primarily involves Python, utilizing essential libraries such as Pandas, Missingno, NLTK, and Seaborn."
  },
  {
    "objectID": "datagather.html",
    "href": "datagather.html",
    "title": "Data gathering",
    "section": "",
    "text": "In recent years, the automotive industry has witnessed a transformative shift towards sustainable and eco-friendly transportation solutions, exemplified by the increasing popularity of electric vehicles (EVs). Among the trailblazers in this revolution is Tesla, a pioneering company that has significantly contributed to the rise of electric cars on our roads. As society embraces cleaner energy alternatives, concerns about the safety of electric cars, particularly in comparison to their traditional gas-powered counterparts, have become a focal point of discussion. This study endeavors to conduct a comprehensive analysis of traffic accidents involving electric cars, with a specific emphasis on Tesla vehicles, and draw meaningful comparisons with incidents involving traditional gas-powered cars. By examining relevant data, trends, and contributing factors, this research aims to shed light on the safety landscape of electric vehicles, providing valuable insights for both policymakers and the general public as we navigate the evolving landscape of transportation technologies."
  },
  {
    "objectID": "datagather.html#introduction",
    "href": "datagather.html#introduction",
    "title": "Data gathering",
    "section": "",
    "text": "In recent years, the automotive industry has witnessed a transformative shift towards sustainable and eco-friendly transportation solutions, exemplified by the increasing popularity of electric vehicles (EVs). Among the trailblazers in this revolution is Tesla, a pioneering company that has significantly contributed to the rise of electric cars on our roads. As society embraces cleaner energy alternatives, concerns about the safety of electric cars, particularly in comparison to their traditional gas-powered counterparts, have become a focal point of discussion. This study endeavors to conduct a comprehensive analysis of traffic accidents involving electric cars, with a specific emphasis on Tesla vehicles, and draw meaningful comparisons with incidents involving traditional gas-powered cars. By examining relevant data, trends, and contributing factors, this research aims to shed light on the safety landscape of electric vehicles, providing valuable insights for both policymakers and the general public as we navigate the evolving landscape of transportation technologies."
  },
  {
    "objectID": "datagather.html#summary-of-the-data",
    "href": "datagather.html#summary-of-the-data",
    "title": "Data gathering",
    "section": "Summary of the data:",
    "text": "Summary of the data:\n\nCase #: Unique identifier for each case. (String)\nYear: Year of the accident. (Integer)\nDate: Date of the accident. (Date)\nCountry: The country where the accident occurred. (String)\nState: State where the accident occurred. (String)\nDescription: Description of the accident. (String)\nDeaths: Number of Deaths (Int)\nTesla driver: Whether the Tesla driver was killed in the accident. (Boolean)\nTesla occupant: Whether a Tesla occupant was killed in the accident. (Boolean)\nOther Vehicle: whether the Tesla crashed another vehicle (Boolean)\nCyclist/ Peds: Whether Tesla killed a Cyclist/Pedestrian in the accident. (Boolean)\nSLA + cycl / peds: Tesla + Cycle / Pedestrian (Boolean)\nAutopilot claimed: People who have claimed Auto Pilot (Boolean)\nVerified Tesla Autopilot Death: Verified Tesla Autopilot Death (Boolean)\nVerified Tesla Autopilot Death & All Deaths Reported to NHTSA SGO : Verified Tesla\nAutopilot Death & All Deaths Reported to NHTSA SGO (Boolean)\nSource: Source of the data. (String)\nSource: Source of the data. (String)\nSource: Source of the data. (String)\nNote: Note (String)\nModel: Model of the Tesla vehicle involved in the accident. (String)\nSource: Source of the data. (String)\nDeceased 1: 1st Dead person’s Name (String)\nDeceased 2: 2nd Dead person’s Name (String)\nDeceased 3: 3rd Dead person’s Name (String)\nDeceased 4: 4th Dead person’s Name (String)\n\n\nQuick exploration of the data:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n\n\nOverview of the dataset\n\n\nCode\ndf = pd.read_csv(\"./Data/Tesla Deaths - Deaths.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nCase #\nYear\nDate\nCountry\nState\nDescription\nDeaths\nTesla driver\nTesla occupant\nOther vehicle\n...\nVerified Tesla Autopilot Deaths\nVerified Tesla Autopilot Deaths + All Deaths Reported to NHTSA SGO\nUnnamed: 16\nUnnamed: 17\nSource\nNote\nDeceased 1\nDeceased 2\nDeceased 3\nDeceased 4\n\n\n\n\n0\n294.0\n2022.0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20230118162813/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n293.0\n2022.0\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20230109041434/ht...\nNaN\nTaren Singh Lal\nNaN\nNaN\nNaN\n\n\n2\n292.0\n2022.0\n1/7/2023\nUSA\nWA\nTesla hits pole, catches on fire\n1.0\n-\n1\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20230107232745/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n291.0\n2022.0\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n290.0\n2022.0\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n-\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221223203725/ht...\nhttps://web.archive.org/web/20221223203725/ht...\nhttps://web.archive.org/web/20221223203725/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 24 columns"
  },
  {
    "objectID": "datagather.html#summary",
    "href": "datagather.html#summary",
    "title": "Data gathering",
    "section": "Summary:",
    "text": "Summary:\nThe UK government collects and publishes (usually on an annual basis) detailed information about traffic accidents across the country. This information includes, but is not limited to, geographical locations, weather conditions, type of vehicles, number of casualties and vehicle manoeuvres, making this a very interesting and comprehensive dataset for analysis and research.\nThe creation of this dataset was inspired by the one previously published by Dave Fisher-Hickey. However, this current dataset features the following significant improvements over its predecessor:\n\nIt covers a wider date range of events.\nMost of the coded data variables have been transformed to textual strings using relevant lookup tables, enabling more efficient and “human-readable” analysis.\nIt features detailed information about the vehicles involved in the accidents.\n\n\nOverview of the dataset\n\n\nCode\ndata = pd.read_csv('./Data/RoadAccident.csv')\ndata.drop(['Accident_Index','Datetime'], axis = 1,inplace=True) \ndata\n\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nRegion\nUrban_or_Rural_Area\nX1st_Road_Class\nDriver_IMD_Decile\nSpeed_limit\nRoad_Type\nRoad_Surface_Conditions\nWeather\n...\nJunction_Detail\nJunction_Location\nX1st_Point_of_Impact\nDriver_Journey_Purpose\nEngine_CC\nPropulsion_Code\nVehicle_Make\nVehicle_Category\nVehicle_Manoeuvre\nAccident_Severity\n\n\n\n\n0\n51.495653\n-0.179097\nLondon\nUrban\nC\n7\n30\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1781\nPetrol\nAudi\nCar\nGoing ahead\nSlight\n\n\n1\n51.499635\n-0.209915\nLondon\nUrban\nA\n3\n30\nSingle carriageway\nDry\nFine\n...\nMore than 4 arms (not roundabout)\nMid Junction - on roundabout or on main road\nOffside\nOther/Not known\n2987\nHeavy oil\nMercedes\nCar\nWaiting to go\nSlight\n\n\n2\n51.492515\n-0.168130\nLondon\nUrban\nUnclassified\n5\n30\nSingle carriageway\nDry\nFine\n...\nCrossroads\nMid Junction - on roundabout or on main road\nFront\nJourney as part of work\n998\nPetrol\nNissan\nCar\nGoing ahead\nSlight\n\n\n3\n51.504784\n-0.193863\nLondon\nUrban\nA\n2\n30\nSingle carriageway\nDry\nFine\n...\nT or staggered junction\nMid Junction - on roundabout or on main road\nOffside\nJourney as part of work\n2179\nHeavy oil\nCitroen\nVan\nTurning right\nSlight\n\n\n4\n51.522072\n-0.212927\nLondon\nUrban\nB\n3\n30\nSingle carriageway\nWet or damp\nFine\n...\nT or staggered junction\nApproaching junction or waiting/parked at junc...\nNearside\nJourney as part of work\n2198\nHeavy oil\nFord\nVan\nOvertaking\nSlight\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n75545\n56.531008\n-2.945169\nScotland\nRural\nA\n9\n70\nDual carriageway\nWet or damp\nRaining\n...\nT or staggered junction\nApproaching junction or waiting/parked at junc...\nFront\nOther/Not known\n2199\nHeavy oil\nKia\nCar\nGoing ahead\nFatal_Serious\n\n\n75546\n56.677867\n-3.688719\nScotland\nRural\nA\n10\n70\nDual carriageway\nDry\nFine\n...\nT or staggered junction\nCleared junction or waiting/parked at junction...\nFront\nOther/Not known\n1598\nPetrol\nVauxhall\nCar\nGoing ahead\nFatal_Serious\n\n\n75547\n55.720385\n-2.654035\nScotland\nRural\nA\n9\n60\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1598\nHeavy oil\nAudi\nCar\nGoing ahead\nFatal_Serious\n\n\n75548\n54.850068\n-4.925632\nScotland\nRural\nB\n5\n60\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1000\nPetrol\nBMW\nMotorcycle\nGoing ahead\nFatal_Serious\n\n\n75549\n55.158556\n-4.195310\nScotland\nRural\nA\n2\n60\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nJourney as part of work\n2143\nHeavy oil\nMercedes\nVan\nGoing ahead\nFatal_Serious\n\n\n\n\n75550 rows × 31 columns\n\n\n\n\n\nCheck for the dimension of the data set\n\n\nCode\nprint('There are a total of {} rows and {} columns in the original dataset'.format(data.shape[0],data.shape[1]))\n\n\nThere are a total of 75550 rows and 31 columns in the original dataset\n\n\n\n\nCheck for null values\n\n\nCode\nprint(\"Any null values in the original dataset?: {}\".format(data.isnull().values.any()))\n\n\nAny null values in the original dataset?: False"
  },
  {
    "objectID": "datagather.html#third-dataset-acquired-using-news-api-and-focused-on-the-topic-of-tesla-related-accident",
    "href": "datagather.html#third-dataset-acquired-using-news-api-and-focused-on-the-topic-of-tesla-related-accident",
    "title": "Data gathering",
    "section": "Third dataset (acquired using News-API and focused on the topic of ‘Tesla-related accident’)",
    "text": "Third dataset (acquired using News-API and focused on the topic of ‘Tesla-related accident’)"
  },
  {
    "objectID": "datagather.html#summary-1",
    "href": "datagather.html#summary-1",
    "title": "Data gathering",
    "section": "Summary:",
    "text": "Summary:\nI obtained the ensuing dataset by utilizing the News-API to search for articles pertaining to Tesla accidents. I also processed and curated the articles, saving them in a structured dataset for subsequent analysis. Furthermore, I generated a word cloud to see prominent keywords within the corpus.\n\n\nCode\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nAPI_KEY='2a0218c20cdd48d29d44f1271c267a25'\nbaseURL = \"https://newsapi.org/v2/everything?\"\n\ntotal_requests= 20\nverbose=True\nTOPIC='Tesla accident'\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nresponse = requests.get(baseURL, URLpost)  \nresponse = response.json() \n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\nwith open(timestamp+'-newapi-raw-data.json', 'w') as file_georetown:\n    json.dump(response, file_georetown, indent=4)\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\narticle_list=response['articles']   \narticle_keys=article_list[0].keys()\nindex=0\ncleaned_data1=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n           #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n               print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data1.append(tmp)\n    index+=1\n\ndf = pd.DataFrame(cleaned_data1).iloc[:, [2, 5]]\ndf.columns = [\"title\", \"content\"]\ndf.to_csv('./Data/newapiTesla.csv', index=False)\n\n\n\n\nCode\npd.DataFrame(cleaned_data1)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nengadget\nnathan ingraham\ntesla's cybertruck is a dystopian, masturbator...\nits been four years since tesla first announce...\n2023-11-30T22:56:48Z\nits been four years since tesla first announce...\n\n\n1\nboing boing\nnatalie dressed\ntesla knew about defects in \"self-driving\" system\na recent fatal accident involving tesla's auto...\n2023-11-25T19:54:09Z\na recent fatal accident involving tesla's auto...\n\n\n2\nreadwrite\nsam shedden\ntesla in fresh legal battle over self-driving ...\nan american judge has suggested elon musk, amo...\n2023-11-23T13:49:29Z\nan american judge has suggested elon musk, amo...\n\n\n3\nautoblog\njonathon ramsey\nwatch as submerged tesla model x at florida bo...\nfiled under: video,green,weird car news,tesla,...\n2023-11-16T15:13:00Z\nthis incident from october is still missing so...\n\n\n4\nautoblog\nreuters\ntesla launched its own car insurance these dri...\nfiled under: green,tesla,insurance,ownership,e...\n2023-11-21T16:00:00Z\nin february, mark bova purchased a used 2018 t...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\nautocar\nNA\nsmart #1: find out why it's a what car? five-s...\nperformance meets practicality in smart's awar...\n2023-12-01T13:30:00Z\ntheres a step-by-step user guide, and an ai vi...\n\n\n96\nwhistleblowersblogorg\nsophie luskin\nmisreporting of tesla safety violations highli...\nleaked safety concerns around technology safet...\n2023-11-17T00:10:46Z\nleaked safety concerns around technology safet...\n\n\n97\n20 minutes\nNA\nspacex a dissimulé pendant 9 ans la mort dun d...\n600 blessures provoquées par des accidents du ...\n2023-11-17T16:02:36Z\nen juin 2014, un employé de spacex âgé de 38 a...\n\n\n98\nelectrek\nfred lambert\ntesla wins public apology from owner who torme...\ntesla has won a court order against an owner i...\n2023-11-23T14:14:18Z\ntesla has won a court order against an owner i...\n\n\n99\nnew york post\nmichael kaplan\ndavid blaine reveals new stunts for vegas show...\n“kissing a king cobra's head is the most dange...\n2023-11-25T13:00:00Z\nsuperstar magician david blaine, naturally, do...\n\n\n\n\n100 rows × 6 columns\n\n\n\n\n\nCode\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud) \n        plt.axis(\"off\")\n\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\ndf = pd.read_csv(\"./Data/newapiTesla.csv\")\ntext = df.iat[0,1] + df.iat[1,1] + df.iat[2,1]\ngenerate_word_cloud(text)"
  }
]