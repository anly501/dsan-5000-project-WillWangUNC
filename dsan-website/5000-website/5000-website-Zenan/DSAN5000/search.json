[
  {
    "objectID": "naive.html",
    "href": "naive.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a simple yet powerful probabilistic machine learning algorithm used for classification and, to some extent, regression tasks. It is based on Bayes’ theorem, which is a fundamental concept in probability theory. Despite its simplicity, Naive Bayes often performs well in various real-world applications and is particularly effective for text classification tasks, such as spam filtering and sentiment analysis.\nThe “naive” in Naive Bayes comes from the assumption of independence among features. It assumes that the presence or absence of a particular feature in a class is independent of the presence or absence of other features. This assumption simplifies the model and makes it computationally efficient, especially when dealing with a large number of features.\nThe algorithm is commonly used in natural language processing (NLP) tasks, where each term in a document is treated as a feature. The Naive Bayes classifier calculates the probability of a document belonging to a particular class based on the probabilities of individual terms occurring in that class. The class with the highest probability is then assigned to the document.\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport warnings \nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom numpy.random import seed # To get reproducible results\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.svm import LinearSVC\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.random import set_seed # Tensorflow seed\nset_seed(18)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nimport pickle\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n\n\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nprint(\"The dataset contains {} different data types and they are: {}\".format(len(column_datatypes), \", \".join(column_datatypes)))\nX = data.drop(columns='Accident_Severity')\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\ny = data['Accident_Severity']\nX = X.drop(columns=['Accident_Index','Datetime'])\ndiscrete_features = ['Driver_IMD_Decile','Speed_limit','Year','Season','Month_of_Year','Day_of_Month','Day_of_Week','Number_of_Vehicles','Age_of_Driver']\nfor item in discrete_features:\n    numerical_features.remove(item)\n    categorical_features.append(item)\nnumerical_features.remove('Accident_Index')\ncategorical_features.remove('Datetime')\n\nThe dataset contains 3 different data types and they are: object, int64, float64\n\n\n\nX.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nRegion\nUrban_or_Rural_Area\nX1st_Road_Class\nDriver_IMD_Decile\nSpeed_limit\nRoad_Type\nRoad_Surface_Conditions\nWeather\nHigh_Wind\nLights\nYear\nSeason\nMonth_of_Year\nDay_of_Month\nDay_of_Week\nHour_of_Day\nNumber_of_Vehicles\nAge_of_Driver\nAge_of_Vehicle\nJunction_Detail\nJunction_Location\nX1st_Point_of_Impact\nDriver_Journey_Purpose\nEngine_CC\nPropulsion_Code\nVehicle_Make\nVehicle_Category\nVehicle_Manoeuvre\n\n\n\n\n0\n51.495653\n-0.179097\nLondon\nUrban\nC\n7\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n1\n19\n2\n0.729\n2\n7\n8\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1781\nPetrol\nAudi\nCar\nGoing ahead\n\n\n1\n51.499635\n-0.209915\nLondon\nUrban\nA\n3\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n2\n8\n1\n0.475\n2\n5\n2\nMore than 4 arms (not roundabout)\nMid Junction - on roundabout or on main road\nOffside\nOther/Not known\n2987\nHeavy oil\nMercedes\nCar\nWaiting to go\n\n\n2\n51.492515\n-0.168130\nLondon\nUrban\nUnclassified\n5\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n3\n3\n3\n0.267\n2\n5\n11\nCrossroads\nMid Junction - on roundabout or on main road\nFront\nJourney as part of work\n998\nPetrol\nNissan\nCar\nGoing ahead\n\n\n3\n51.504784\n-0.193863\nLondon\nUrban\nA\n2\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n3\n4\n4\n0.566\n2\n3\n5\nT or staggered junction\nMid Junction - on roundabout or on main road\nOffside\nJourney as part of work\n2179\nHeavy oil\nCitroen\nVan\nTurning right\n\n\n4\n51.522072\n-0.212927\nLondon\nUrban\nB\n3\n30\nSingle carriageway\nWet or damp\nFine\nNo\nDaylight\n2010\n4\n3\n12\n5\n0.670\n1\n4\n4\nT or staggered junction\nApproaching junction or waiting/parked at junc...\nNearside\nJourney as part of work\n2198\nHeavy oil\nFord\nVan\nOvertaking\n\n\n\n\n\n\n\n\ny.head()\n\n0    Slight\n1    Slight\n2    Slight\n3    Slight\n4    Slight\nName: Accident_Severity, dtype: object\n\n\n\n\n\n\n\n\n\ncategorical_features_index = list()\nfor i in categorical_features:\n    categorical_features_index.append(X.columns.get_loc(i))\n    X[i] = LabelEncoder().fit_transform(X[i])\nX.head()\ny = LabelEncoder().fit_transform(y)\n\n\n\n\n\nX_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.25, random_state = 18)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.2, random_state = 18)\n\n\n\n\n\ndefine the hyperparameters and the best alpha value\n\n\nmixed_nb_alpha = [1.0,1.5,2.0,2.5,3.0,3.5,4.0]\nmixedNB_validation_misclassification_rates = list()\nfor alpha in mixed_nb_alpha:\n    mixedNB = GaussianNB()\n    mixedNB.fit(X_train,y_train)\n    y_hat_valid = mixedNB.predict(X_valid)\n    mixedNB_validation_misclassification_rates.append(sum(y_hat_valid != y_valid)/ len(y_valid))\n\n\nfind the best k value for the model\n\n\nbestK_mixedNB = np.argmin(mixedNB_validation_misclassification_rates) + 1\nprint(\"The best K value is: {}\".format(bestK_mixedNB))\n\nThe best K value is: 1\n\n\n\nrefit the Gaussian Naive Bayes model with the found parameters\n\n\nmixedNB_best_model = GaussianNB()\nmixedNB_best_model.fit(X_train_valid,y_train_valid)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\ncalculate the classification accuracy on the testing set\n\n\nfrom sklearn import metrics\ny_pred = mixedNB_best_model.predict(X_test)\nprint(\"Classification Report:\")\nprint(metrics.classification_report(y_test, y_pred))\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(\"The classification accuracy on the testing set is: {:.2%}\".format(np.mean(mixedNB_best_model.predict(X_test) == y_test)))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.42      0.23      0.30      4712\n           1       0.78      0.89      0.83     14176\n\n    accuracy                           0.73     18888\n   macro avg       0.60      0.56      0.57     18888\nweighted avg       0.69      0.73      0.70     18888\n\nThe classification accuracy on the testing set is: 72.92%\n\n\n\n\n\n\nPlot the confusion matrix\n\n\ncm = metrics.confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\nPlot the ROC curve\n\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, mixedNB_best_model.predict_proba(X_test)[:, 1])\nplt.plot(fpr, tpr, label=\"ROC Curve\")\nplt.plot([0, 1], [0, 1], \"--\", label=\"Random Guessing\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "naive.html#naïve-bayes-on-the-second-dataset-traffic-accidents-and-vehicles-gas-car",
    "href": "naive.html#naïve-bayes-on-the-second-dataset-traffic-accidents-and-vehicles-gas-car",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a simple yet powerful probabilistic machine learning algorithm used for classification and, to some extent, regression tasks. It is based on Bayes’ theorem, which is a fundamental concept in probability theory. Despite its simplicity, Naive Bayes often performs well in various real-world applications and is particularly effective for text classification tasks, such as spam filtering and sentiment analysis.\nThe “naive” in Naive Bayes comes from the assumption of independence among features. It assumes that the presence or absence of a particular feature in a class is independent of the presence or absence of other features. This assumption simplifies the model and makes it computationally efficient, especially when dealing with a large number of features.\nThe algorithm is commonly used in natural language processing (NLP) tasks, where each term in a document is treated as a feature. The Naive Bayes classifier calculates the probability of a document belonging to a particular class based on the probabilities of individual terms occurring in that class. The class with the highest probability is then assigned to the document.\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport warnings \nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom numpy.random import seed # To get reproducible results\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.svm import LinearSVC\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.random import set_seed # Tensorflow seed\nset_seed(18)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nimport pickle\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n\n\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nprint(\"The dataset contains {} different data types and they are: {}\".format(len(column_datatypes), \", \".join(column_datatypes)))\nX = data.drop(columns='Accident_Severity')\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\ny = data['Accident_Severity']\nX = X.drop(columns=['Accident_Index','Datetime'])\ndiscrete_features = ['Driver_IMD_Decile','Speed_limit','Year','Season','Month_of_Year','Day_of_Month','Day_of_Week','Number_of_Vehicles','Age_of_Driver']\nfor item in discrete_features:\n    numerical_features.remove(item)\n    categorical_features.append(item)\nnumerical_features.remove('Accident_Index')\ncategorical_features.remove('Datetime')\n\nThe dataset contains 3 different data types and they are: object, int64, float64\n\n\n\nX.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nRegion\nUrban_or_Rural_Area\nX1st_Road_Class\nDriver_IMD_Decile\nSpeed_limit\nRoad_Type\nRoad_Surface_Conditions\nWeather\nHigh_Wind\nLights\nYear\nSeason\nMonth_of_Year\nDay_of_Month\nDay_of_Week\nHour_of_Day\nNumber_of_Vehicles\nAge_of_Driver\nAge_of_Vehicle\nJunction_Detail\nJunction_Location\nX1st_Point_of_Impact\nDriver_Journey_Purpose\nEngine_CC\nPropulsion_Code\nVehicle_Make\nVehicle_Category\nVehicle_Manoeuvre\n\n\n\n\n0\n51.495653\n-0.179097\nLondon\nUrban\nC\n7\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n1\n19\n2\n0.729\n2\n7\n8\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1781\nPetrol\nAudi\nCar\nGoing ahead\n\n\n1\n51.499635\n-0.209915\nLondon\nUrban\nA\n3\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n2\n8\n1\n0.475\n2\n5\n2\nMore than 4 arms (not roundabout)\nMid Junction - on roundabout or on main road\nOffside\nOther/Not known\n2987\nHeavy oil\nMercedes\nCar\nWaiting to go\n\n\n2\n51.492515\n-0.168130\nLondon\nUrban\nUnclassified\n5\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n3\n3\n3\n0.267\n2\n5\n11\nCrossroads\nMid Junction - on roundabout or on main road\nFront\nJourney as part of work\n998\nPetrol\nNissan\nCar\nGoing ahead\n\n\n3\n51.504784\n-0.193863\nLondon\nUrban\nA\n2\n30\nSingle carriageway\nDry\nFine\nNo\nDaylight\n2010\n4\n3\n4\n4\n0.566\n2\n3\n5\nT or staggered junction\nMid Junction - on roundabout or on main road\nOffside\nJourney as part of work\n2179\nHeavy oil\nCitroen\nVan\nTurning right\n\n\n4\n51.522072\n-0.212927\nLondon\nUrban\nB\n3\n30\nSingle carriageway\nWet or damp\nFine\nNo\nDaylight\n2010\n4\n3\n12\n5\n0.670\n1\n4\n4\nT or staggered junction\nApproaching junction or waiting/parked at junc...\nNearside\nJourney as part of work\n2198\nHeavy oil\nFord\nVan\nOvertaking\n\n\n\n\n\n\n\n\ny.head()\n\n0    Slight\n1    Slight\n2    Slight\n3    Slight\n4    Slight\nName: Accident_Severity, dtype: object\n\n\n\n\n\n\n\n\n\ncategorical_features_index = list()\nfor i in categorical_features:\n    categorical_features_index.append(X.columns.get_loc(i))\n    X[i] = LabelEncoder().fit_transform(X[i])\nX.head()\ny = LabelEncoder().fit_transform(y)\n\n\n\n\n\nX_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size = 0.25, random_state = 18)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.2, random_state = 18)\n\n\n\n\n\ndefine the hyperparameters and the best alpha value\n\n\nmixed_nb_alpha = [1.0,1.5,2.0,2.5,3.0,3.5,4.0]\nmixedNB_validation_misclassification_rates = list()\nfor alpha in mixed_nb_alpha:\n    mixedNB = GaussianNB()\n    mixedNB.fit(X_train,y_train)\n    y_hat_valid = mixedNB.predict(X_valid)\n    mixedNB_validation_misclassification_rates.append(sum(y_hat_valid != y_valid)/ len(y_valid))\n\n\nfind the best k value for the model\n\n\nbestK_mixedNB = np.argmin(mixedNB_validation_misclassification_rates) + 1\nprint(\"The best K value is: {}\".format(bestK_mixedNB))\n\nThe best K value is: 1\n\n\n\nrefit the Gaussian Naive Bayes model with the found parameters\n\n\nmixedNB_best_model = GaussianNB()\nmixedNB_best_model.fit(X_train_valid,y_train_valid)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\ncalculate the classification accuracy on the testing set\n\n\nfrom sklearn import metrics\ny_pred = mixedNB_best_model.predict(X_test)\nprint(\"Classification Report:\")\nprint(metrics.classification_report(y_test, y_pred))\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(\"The classification accuracy on the testing set is: {:.2%}\".format(np.mean(mixedNB_best_model.predict(X_test) == y_test)))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.42      0.23      0.30      4712\n           1       0.78      0.89      0.83     14176\n\n    accuracy                           0.73     18888\n   macro avg       0.60      0.56      0.57     18888\nweighted avg       0.69      0.73      0.70     18888\n\nThe classification accuracy on the testing set is: 72.92%\n\n\n\n\n\n\nPlot the confusion matrix\n\n\ncm = metrics.confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\nPlot the ROC curve\n\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, mixedNB_best_model.predict_proba(X_test)[:, 1])\nplt.plot(fpr, tpr, label=\"ROC Curve\")\nplt.plot([0, 1], [0, 1], \"--\", label=\"Random Guessing\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "dimreduction.html",
    "href": "dimreduction.html",
    "title": "Dimensionality reduction",
    "section": "",
    "text": "Dimensionality reduction is a technique employed in data analysis and machine learning to reduce the number of features or variables in a dataset while preserving its essential information. High-dimensional datasets with many features can pose challenges, such as increased computational complexity, potential overfitting, and difficulty in visualization. Dimensionality reduction methods aim to address these issues by transforming the data into a lower-dimensional representation.\nTwo common approaches to dimensionality reduction are:\n\n\n\nFeature selection involves choosing a subset of the original features.\nMethods may be filter-based (e.g., based on statistical tests), wrapper-based (e.g., using models to evaluate subsets), or embedded (e.g., incorporated into the model training process).\nSelected features are retained, and others are discarded.\n\n\n\n\n\nFeature extraction creates new features as combinations of the original ones.\nPrincipal Component Analysis (PCA) is a widely used linear technique that transforms the data into a set of uncorrelated features called principal components.\nOther methods, such as t-distributed Stochastic Neighbor Embedding (t-SNE) or Uniform Manifold Approximation and Projection (UMAP), are nonlinear and can capture complex relationships in the data."
  },
  {
    "objectID": "dimreduction.html#brife-introduction",
    "href": "dimreduction.html#brife-introduction",
    "title": "Dimensionality reduction",
    "section": "",
    "text": "Dimensionality reduction is a technique employed in data analysis and machine learning to reduce the number of features or variables in a dataset while preserving its essential information. High-dimensional datasets with many features can pose challenges, such as increased computational complexity, potential overfitting, and difficulty in visualization. Dimensionality reduction methods aim to address these issues by transforming the data into a lower-dimensional representation.\nTwo common approaches to dimensionality reduction are:\n\n\n\nFeature selection involves choosing a subset of the original features.\nMethods may be filter-based (e.g., based on statistical tests), wrapper-based (e.g., using models to evaluate subsets), or embedded (e.g., incorporated into the model training process).\nSelected features are retained, and others are discarded.\n\n\n\n\n\nFeature extraction creates new features as combinations of the original ones.\nPrincipal Component Analysis (PCA) is a widely used linear technique that transforms the data into a set of uncorrelated features called principal components.\nOther methods, such as t-distributed Stochastic Neighbor Embedding (t-SNE) or Uniform Manifold Approximation and Projection (UMAP), are nonlinear and can capture complex relationships in the data."
  },
  {
    "objectID": "dimreduction.html#using-pca-on-the-dataset-tesla-deaths---deaths",
    "href": "dimreduction.html#using-pca-on-the-dataset-tesla-deaths---deaths",
    "title": "Dimensionality reduction",
    "section": "Using PCA on the dataset: Tesla Deaths - Deaths",
    "text": "Using PCA on the dataset: Tesla Deaths - Deaths\n\nImport necessary libraries\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nCreate the features and target dataframes and plot the proportion of variance explained on the y-axis of the bar plot (composed of the 6 features)\nWe set ‘Tesla driver’ as the target variable, which indicates whether or not the Tesla driver is killed in the accident.\n\ndf = pd.read_csv('./cleandata/cleanTelsa.csv')\nfeatures = df[['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle']]\ntarget = df['Tesla_driver']\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\npca = PCA()\npca.fit(features_scaled)\nplt.bar(range(1, pca.n_components_ + 1),  pca.explained_variance_ratio_)\nplt.xlabel('Principal component #')\nplt.ylabel('Proportion of variance explained')\nplt.xticks([1, 2, 3, 4, 5, 6])\n\n([&lt;matplotlib.axis.XTick at 0x156d797f670&gt;,\n  &lt;matplotlib.axis.XTick at 0x156d797f640&gt;,\n  &lt;matplotlib.axis.XTick at 0x156d797f250&gt;,\n  &lt;matplotlib.axis.XTick at 0x156d324c670&gt;,\n  &lt;matplotlib.axis.XTick at 0x156d324d120&gt;,\n  &lt;matplotlib.axis.XTick at 0x156d324dbd0&gt;],\n [Text(1, 0, '1'),\n  Text(2, 0, '2'),\n  Text(3, 0, '3'),\n  Text(4, 0, '4'),\n  Text(5, 0, '5'),\n  Text(6, 0, '6')])\n\n\n\n\n\n\n\nCompute the cumulative proportion of variance explained by the first two principal components\n\ntwo_first_comp_var_exp = pca.explained_variance_ratio_.cumsum()[1]\nprint(\"The cumulative variance of the first two principal components is {}\".format(\n    round(two_first_comp_var_exp, 5)))\n\nThe cumulative variance of the first two principal components is 0.60595"
  },
  {
    "objectID": "dimreduction.html#now-lets-visualize-the-first-two-principal-components",
    "href": "dimreduction.html#now-lets-visualize-the-first-two-principal-components",
    "title": "Dimensionality reduction",
    "section": "Now let’s visualize the first two principal components",
    "text": "Now let’s visualize the first two principal components\nThe first two principal components together capture a decent proportion of the variation (60%) from all six features\n\nCreate a scatter plot of the first principle components and explore how the states cluster together\n\npca = PCA(n_components=2)\np_comps = pca.fit_transform(features_scaled)\np_comp1 = p_comps[:,0]\np_comp2 = p_comps[:,1]\nplt.scatter(p_comp1,p_comp2)\nplt.show()"
  },
  {
    "objectID": "dimreduction.html#summary",
    "href": "dimreduction.html#summary",
    "title": "Dimensionality reduction",
    "section": "Summary:",
    "text": "Summary:\nIt was not entirely clear from the PCA scatter plot where the clusters are. However, we do know that there are only two values that ‘Tesla-driver’ variable can take, hence we can cluster the data into 2 groups using techniques such as K-Means."
  },
  {
    "objectID": "dimreduction.html#using-t-sne",
    "href": "dimreduction.html#using-t-sne",
    "title": "Dimensionality reduction",
    "section": "Using t-SNE",
    "text": "Using t-SNE\n\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(features_scaled)\ntsne_df = pd.DataFrame(data = X_tsne, columns = ['tsne component 1', 'tsne component 2'])\nprint(tsne_df.shape)\ntsne_df.head()\n\n(294, 2)\n\n\n\n\n\n\n\n\n\ntsne component 1\ntsne component 2\n\n\n\n\n0\n123.198708\n99.458725\n\n\n1\n161.351639\n139.500549\n\n\n2\n-36.251751\n274.276825\n\n\n3\n115.615074\n-26.885727\n\n\n4\n10.898621\n-296.469269\n\n\n\n\n\n\n\n\nt-SNE plot in 2D coloured by class\n\nplt.figure(figsize=(8,6))\nplt.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], cmap=\"brg\", s=40)\nplt.title('t-SNE plot in 2D')\nplt.xlabel('tsne component 1')\nplt.ylabel('tsne component 2')\n\nC:\\Users\\23898\\AppData\\Local\\Temp\\ipykernel_21272\\977625591.py:2: UserWarning:\n\nNo data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n\n\n\nText(0, 0.5, 'tsne component 2')"
  },
  {
    "objectID": "dimreduction.html#summary-1",
    "href": "dimreduction.html#summary-1",
    "title": "Dimensionality reduction",
    "section": "Summary",
    "text": "Summary\nLike PCA, this plot does not give useful information, since we can not tell where are the two clusters. Clustering methods such as ‘K-Means’ should be used."
  },
  {
    "objectID": "dimreduction.html#using-pca-on-the-dataset-traffic-accidents-and-vehicles-gas-car",
    "href": "dimreduction.html#using-pca-on-the-dataset-traffic-accidents-and-vehicles-gas-car",
    "title": "Dimensionality reduction",
    "section": "Using PCA on the dataset: Traffic Accidents and Vehicles (gas car)",
    "text": "Using PCA on the dataset: Traffic Accidents and Vehicles (gas car)\n\n\nCode\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nX = data.drop(columns='Accident_Severity')\ny = data['Accident_Severity']\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    # In the dataset we only have float and int64.\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    # Categorical\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\ndata = X[numerical_features]\n\n\n\nNow we have a cleaned dataset with 15 numerical features, and we would like to cluster the accidents into smaller groups in terms of its type\n\n\nStart PCA on the dataset with number of components equal to 2\n\nSS=StandardScaler()\nX=pd.DataFrame(SS.fit_transform(data), columns=data.columns)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nprincipal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\nprincipal_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\n\n\n\n\n0\n-1.949234\n-1.672969\n\n\n1\n-2.311867\n-1.581080\n\n\n2\n-1.894229\n-0.578888\n\n\n3\n-2.267190\n-0.318268\n\n\n4\n-2.164234\n-0.289284\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,6))\nplt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], s=0.05)\nplt.title('PCA plot in 2D')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\nText(0, 0.5, 'PC2')\n\n\n\n\n\n\n\nIn summary:\nThe depicted group lacks a discernible clustering pattern. To address this, we need to employ a clustering method to delineate distinct clusters within the data.\n\nkmeans = KMeans(n_clusters=5, n_init=15, max_iter=500, random_state=0)\nclusters = kmeans.fit_predict(X)\ncentroids = kmeans.cluster_centers_\ncentroids_pca = pca.transform(centroids)\nplt.figure(figsize=(8,6))\nplt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], c=clusters, cmap=\"brg\", s=40)\nplt.scatter(x=centroids_pca[:,0], y=centroids_pca[:,1], marker=\"x\", s=100, linewidths=3, color=\"black\")\nplt.title('PCA plot in 2D')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\base.py:464: UserWarning:\n\nX does not have valid feature names, but PCA was fitted with feature names\n\n\n\nText(0, 0.5, 'PC2')"
  },
  {
    "objectID": "dimreduction.html#conclusion",
    "href": "dimreduction.html#conclusion",
    "title": "Dimensionality reduction",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nPCA and t-SNE may not perform optimally independently and often benefit from auxiliary techniques such as clustering, notably K-Means.\nWhile PCA and t-SNE yield different outputs, the computational efficiency of t-SNE is hindered by its quadratic time complexity and stochastic nature. Therefore, in scenarios where efficiency is crucial, PCA might be the preferred choice for dimension reduction."
  },
  {
    "objectID": "datagather.html",
    "href": "datagather.html",
    "title": "Data gathering",
    "section": "",
    "text": "In recent years, the automotive industry has witnessed a transformative shift towards sustainable and eco-friendly transportation solutions, exemplified by the increasing popularity of electric vehicles (EVs). Among the trailblazers in this revolution is Tesla, a pioneering company that has significantly contributed to the rise of electric cars on our roads. As society embraces cleaner energy alternatives, concerns about the safety of electric cars, particularly in comparison to their traditional gas-powered counterparts, have become a focal point of discussion. This study endeavors to conduct a comprehensive analysis of traffic accidents involving electric cars, with a specific emphasis on Tesla vehicles, and draw meaningful comparisons with incidents involving traditional gas-powered cars. By examining relevant data, trends, and contributing factors, this research aims to shed light on the safety landscape of electric vehicles, providing valuable insights for both policymakers and the general public as we navigate the evolving landscape of transportation technologies."
  },
  {
    "objectID": "datagather.html#introduction",
    "href": "datagather.html#introduction",
    "title": "Data gathering",
    "section": "",
    "text": "In recent years, the automotive industry has witnessed a transformative shift towards sustainable and eco-friendly transportation solutions, exemplified by the increasing popularity of electric vehicles (EVs). Among the trailblazers in this revolution is Tesla, a pioneering company that has significantly contributed to the rise of electric cars on our roads. As society embraces cleaner energy alternatives, concerns about the safety of electric cars, particularly in comparison to their traditional gas-powered counterparts, have become a focal point of discussion. This study endeavors to conduct a comprehensive analysis of traffic accidents involving electric cars, with a specific emphasis on Tesla vehicles, and draw meaningful comparisons with incidents involving traditional gas-powered cars. By examining relevant data, trends, and contributing factors, this research aims to shed light on the safety landscape of electric vehicles, providing valuable insights for both policymakers and the general public as we navigate the evolving landscape of transportation technologies."
  },
  {
    "objectID": "datagather.html#summary-of-the-data",
    "href": "datagather.html#summary-of-the-data",
    "title": "Data gathering",
    "section": "Summary of the data:",
    "text": "Summary of the data:\n\nCase #: Unique identifier for each case. (String)\nYear: Year of the accident. (Integer)\nDate: Date of the accident. (Date)\nCountry: The country where the accident occurred. (String)\nState: State where the accident occurred. (String)\nDescription: Description of the accident. (String)\nDeaths: Number of Deaths (Int)\nTesla driver: Whether the Tesla driver was killed in the accident. (Boolean)\nTesla occupant: Whether a Tesla occupant was killed in the accident. (Boolean)\nOther Vehicle: whether the Tesla crashed another vehicle (Boolean)\nCyclist/ Peds: Whether Tesla killed a Cyclist/Pedestrian in the accident. (Boolean)\nSLA + cycl / peds: Tesla + Cycle / Pedestrian (Boolean)\nAutopilot claimed: People who have claimed Auto Pilot (Boolean)\nVerified Tesla Autopilot Death: Verified Tesla Autopilot Death (Boolean)\nVerified Tesla Autopilot Death & All Deaths Reported to NHTSA SGO : Verified Tesla\nAutopilot Death & All Deaths Reported to NHTSA SGO (Boolean)\nSource: Source of the data. (String)\nSource: Source of the data. (String)\nSource: Source of the data. (String)\nNote: Note (String)\nModel: Model of the Tesla vehicle involved in the accident. (String)\nSource: Source of the data. (String)\nDeceased 1: 1st Dead person’s Name (String)\nDeceased 2: 2nd Dead person’s Name (String)\nDeceased 3: 3rd Dead person’s Name (String)\nDeceased 4: 4th Dead person’s Name (String)\n\n\nQuick exploration of the data:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n\n\nOverview of the dataset\n\n\nCode\ndf = pd.read_csv(\"./Data/Tesla Deaths - Deaths.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nCase #\nYear\nDate\nCountry\nState\nDescription\nDeaths\nTesla driver\nTesla occupant\nOther vehicle\n...\nVerified Tesla Autopilot Deaths\nVerified Tesla Autopilot Deaths + All Deaths Reported to NHTSA SGO\nUnnamed: 16\nUnnamed: 17\nSource\nNote\nDeceased 1\nDeceased 2\nDeceased 3\nDeceased 4\n\n\n\n\n0\n294.0\n2022.0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20230118162813/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n293.0\n2022.0\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20230109041434/ht...\nNaN\nTaren Singh Lal\nNaN\nNaN\nNaN\n\n\n2\n292.0\n2022.0\n1/7/2023\nUSA\nWA\nTesla hits pole, catches on fire\n1.0\n-\n1\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20230107232745/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n291.0\n2022.0\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nhttps://web.archive.org/web/20221222203930/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n290.0\n2022.0\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n-\n-\n-\n...\n-\n-\nhttps://web.archive.org/web/20221223203725/ht...\nhttps://web.archive.org/web/20221223203725/ht...\nhttps://web.archive.org/web/20221223203725/ht...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 24 columns"
  },
  {
    "objectID": "datagather.html#summary",
    "href": "datagather.html#summary",
    "title": "Data gathering",
    "section": "Summary:",
    "text": "Summary:\nThe UK government collects and publishes (usually on an annual basis) detailed information about traffic accidents across the country. This information includes, but is not limited to, geographical locations, weather conditions, type of vehicles, number of casualties and vehicle manoeuvres, making this a very interesting and comprehensive dataset for analysis and research.\nThe creation of this dataset was inspired by the one previously published by Dave Fisher-Hickey. However, this current dataset features the following significant improvements over its predecessor:\n\nIt covers a wider date range of events.\nMost of the coded data variables have been transformed to textual strings using relevant lookup tables, enabling more efficient and “human-readable” analysis.\nIt features detailed information about the vehicles involved in the accidents.\n\n\nOverview of the dataset\n\n\nCode\ndata = pd.read_csv('./Data/RoadAccident.csv')\ndata.drop(['Accident_Index','Datetime'], axis = 1,inplace=True) \ndata\n\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nRegion\nUrban_or_Rural_Area\nX1st_Road_Class\nDriver_IMD_Decile\nSpeed_limit\nRoad_Type\nRoad_Surface_Conditions\nWeather\n...\nJunction_Detail\nJunction_Location\nX1st_Point_of_Impact\nDriver_Journey_Purpose\nEngine_CC\nPropulsion_Code\nVehicle_Make\nVehicle_Category\nVehicle_Manoeuvre\nAccident_Severity\n\n\n\n\n0\n51.495653\n-0.179097\nLondon\nUrban\nC\n7\n30\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1781\nPetrol\nAudi\nCar\nGoing ahead\nSlight\n\n\n1\n51.499635\n-0.209915\nLondon\nUrban\nA\n3\n30\nSingle carriageway\nDry\nFine\n...\nMore than 4 arms (not roundabout)\nMid Junction - on roundabout or on main road\nOffside\nOther/Not known\n2987\nHeavy oil\nMercedes\nCar\nWaiting to go\nSlight\n\n\n2\n51.492515\n-0.168130\nLondon\nUrban\nUnclassified\n5\n30\nSingle carriageway\nDry\nFine\n...\nCrossroads\nMid Junction - on roundabout or on main road\nFront\nJourney as part of work\n998\nPetrol\nNissan\nCar\nGoing ahead\nSlight\n\n\n3\n51.504784\n-0.193863\nLondon\nUrban\nA\n2\n30\nSingle carriageway\nDry\nFine\n...\nT or staggered junction\nMid Junction - on roundabout or on main road\nOffside\nJourney as part of work\n2179\nHeavy oil\nCitroen\nVan\nTurning right\nSlight\n\n\n4\n51.522072\n-0.212927\nLondon\nUrban\nB\n3\n30\nSingle carriageway\nWet or damp\nFine\n...\nT or staggered junction\nApproaching junction or waiting/parked at junc...\nNearside\nJourney as part of work\n2198\nHeavy oil\nFord\nVan\nOvertaking\nSlight\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n75545\n56.531008\n-2.945169\nScotland\nRural\nA\n9\n70\nDual carriageway\nWet or damp\nRaining\n...\nT or staggered junction\nApproaching junction or waiting/parked at junc...\nFront\nOther/Not known\n2199\nHeavy oil\nKia\nCar\nGoing ahead\nFatal_Serious\n\n\n75546\n56.677867\n-3.688719\nScotland\nRural\nA\n10\n70\nDual carriageway\nDry\nFine\n...\nT or staggered junction\nCleared junction or waiting/parked at junction...\nFront\nOther/Not known\n1598\nPetrol\nVauxhall\nCar\nGoing ahead\nFatal_Serious\n\n\n75547\n55.720385\n-2.654035\nScotland\nRural\nA\n9\n60\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1598\nHeavy oil\nAudi\nCar\nGoing ahead\nFatal_Serious\n\n\n75548\n54.850068\n-4.925632\nScotland\nRural\nB\n5\n60\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nOther/Not known\n1000\nPetrol\nBMW\nMotorcycle\nGoing ahead\nFatal_Serious\n\n\n75549\n55.158556\n-4.195310\nScotland\nRural\nA\n2\n60\nSingle carriageway\nDry\nFine\n...\nNot at junction or within 20 metres\nNot at or within 20 metres of junction\nFront\nJourney as part of work\n2143\nHeavy oil\nMercedes\nVan\nGoing ahead\nFatal_Serious\n\n\n\n\n75550 rows × 31 columns\n\n\n\n\n\nCheck for the dimension of the data set\n\n\nCode\nprint('There are a total of {} rows and {} columns in the original dataset'.format(data.shape[0],data.shape[1]))\n\n\nThere are a total of 75550 rows and 31 columns in the original dataset\n\n\n\n\nCheck for null values\n\n\nCode\nprint(\"Any null values in the original dataset?: {}\".format(data.isnull().values.any()))\n\n\nAny null values in the original dataset?: False"
  },
  {
    "objectID": "datagather.html#third-dataset-acquired-using-news-api-and-focused-on-the-topic-of-tesla-related-accident",
    "href": "datagather.html#third-dataset-acquired-using-news-api-and-focused-on-the-topic-of-tesla-related-accident",
    "title": "Data gathering",
    "section": "Third dataset (acquired using News-API and focused on the topic of ‘Tesla-related accident’)",
    "text": "Third dataset (acquired using News-API and focused on the topic of ‘Tesla-related accident’)"
  },
  {
    "objectID": "datagather.html#summary-1",
    "href": "datagather.html#summary-1",
    "title": "Data gathering",
    "section": "Summary:",
    "text": "Summary:\nI obtained the ensuing dataset by utilizing the News-API to search for articles pertaining to Tesla accidents. I also processed and curated the articles, saving them in a structured dataset for subsequent analysis. Furthermore, I generated a word cloud to see prominent keywords within the corpus.\n\n\nCode\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nAPI_KEY='2a0218c20cdd48d29d44f1271c267a25'\nbaseURL = \"https://newsapi.org/v2/everything?\"\n\ntotal_requests= 20\nverbose=True\nTOPIC='Tesla accident'\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nresponse = requests.get(baseURL, URLpost)  \nresponse = response.json() \n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\nwith open(timestamp+'-newapi-raw-data.json', 'w') as file_georetown:\n    json.dump(response, file_georetown, indent=4)\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\narticle_list=response['articles']   \narticle_keys=article_list[0].keys()\nindex=0\ncleaned_data1=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n           #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n               print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data1.append(tmp)\n    index+=1\n\ndf = pd.DataFrame(cleaned_data1).iloc[:, [2, 5]]\ndf.columns = [\"title\", \"content\"]\ndf.to_csv('./Data/newapiTesla.csv', index=False)\n\n\n\n\nCode\npd.DataFrame(cleaned_data1)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\nengadget\nnathan ingraham\ntesla's cybertruck is a dystopian, masturbator...\nits been four years since tesla first announce...\n2023-11-30T22:56:48Z\nits been four years since tesla first announce...\n\n\n1\nboing boing\nnatalie dressed\ntesla knew about defects in \"self-driving\" system\na recent fatal accident involving tesla's auto...\n2023-11-25T19:54:09Z\na recent fatal accident involving tesla's auto...\n\n\n2\nreadwrite\nsam shedden\ntesla in fresh legal battle over self-driving ...\nan american judge has suggested elon musk, amo...\n2023-11-23T13:49:29Z\nan american judge has suggested elon musk, amo...\n\n\n3\nautoblog\njonathon ramsey\nwatch as submerged tesla model x at florida bo...\nfiled under: video,green,weird car news,tesla,...\n2023-11-16T15:13:00Z\nthis incident from october is still missing so...\n\n\n4\nautoblog\nreuters\ntesla launched its own car insurance these dri...\nfiled under: green,tesla,insurance,ownership,e...\n2023-11-21T16:00:00Z\nin february, mark bova purchased a used 2018 t...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\nautocar\nNA\nsmart #1: find out why it's a what car? five-s...\nperformance meets practicality in smart's awar...\n2023-12-01T13:30:00Z\ntheres a step-by-step user guide, and an ai vi...\n\n\n96\nwhistleblowersblogorg\nsophie luskin\nmisreporting of tesla safety violations highli...\nleaked safety concerns around technology safet...\n2023-11-17T00:10:46Z\nleaked safety concerns around technology safet...\n\n\n97\n20 minutes\nNA\nspacex a dissimulé pendant 9 ans la mort dun d...\n600 blessures provoquées par des accidents du ...\n2023-11-17T16:02:36Z\nen juin 2014, un employé de spacex âgé de 38 a...\n\n\n98\nelectrek\nfred lambert\ntesla wins public apology from owner who torme...\ntesla has won a court order against an owner i...\n2023-11-23T14:14:18Z\ntesla has won a court order against an owner i...\n\n\n99\nnew york post\nmichael kaplan\ndavid blaine reveals new stunts for vegas show...\n“kissing a king cobra's head is the most dange...\n2023-11-25T13:00:00Z\nsuperstar magician david blaine, naturally, do...\n\n\n\n\n100 rows × 6 columns\n\n\n\n\n\nCode\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud) \n        plt.axis(\"off\")\n\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\ndf = pd.read_csv(\"./Data/newapiTesla.csv\")\ntext = df.iat[0,1] + df.iat[1,1] + df.iat[2,1]\ngenerate_word_cloud(text)"
  },
  {
    "objectID": "dataclean.html",
    "href": "dataclean.html",
    "title": "Data cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv(\"./Data/Tesla Deaths - Deaths.csv\")\n\n\n\n\n\n\nCode\nmsno.heatmap(df)\n\n\n\n\n\n\n\n\n\n\n\nCode\nnew_df = pd.DataFrame()\nfor i in range(len(df.columns[:14]),1,-1):\n    new_df.insert(0,df.columns[i],df[df.columns[i]])\ndf = new_df\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla driver\nTesla occupant\nOther vehicle\nCyclists/ Peds\nTSLA+cycl / peds\nModel\nAutopilot claimed\nVerified Tesla Autopilot Deaths\n\n\n\n\n0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n1\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n2\n1/7/2023\nUSA\nWA\nTesla hits pole, catches on fire\n1.0\n-\n1\n-\n-\n1\n-\n-\n-\n\n\n3\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n4\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n-\n-\n-\n1\n1\n-\n-\n-\n\n\n\n\n\n\n\n\n\nCode\nmsno.bar(df)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\nCode\nfor i in range(5,10):\n    df[df.columns[i]] = df[df.columns[i]].fillna(\"-\")\nfor i in range(11,13):\n    df[df.columns[i]] = df[df.columns[i]].fillna('-')\nmsno.bar(df)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\nCode\ndf = df.dropna()\nmsno.bar(df)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\ndf.columns = ['Date','Country','State','Description','Deaths',\"Tesla_driver\",\"Tesla_occupant\",\"Other_vehicle\",\"CP\",\"tsla+cp\",\"Model\",\"Claimed\",\"VTAD\"]\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla_driver\nTesla_occupant\nOther_vehicle\nCP\ntsla+cp\nModel\nClaimed\nVTAD\n\n\n\n\n0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n1\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n2\n1/7/2023\nUSA\nWA\nTesla hits pole, catches on fire\n1.0\n-\n1\n-\n-\n1\n-\n-\n-\n\n\n3\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n4\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n-\n-\n-\n1\n1\n-\n-\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nprint(\"The dataset contains {} different data types and they are: {}\".format(len(column_datatypes), \", \".join(column_datatypes)))\nX = data.drop(columns='Accident_Severity')\ny = data['Accident_Severity']\ncount = pd.value_counts(y, sort = True)\ncount.plot(kind = 'bar', rot=0)\nplt.title(\"Distribution of Accident Severity \")\nplt.xlabel(\"Result\")\nplt.ylabel(\"Count\")\n\n\nThe dataset contains 3 different data types and they are: float64, object, int64\n\n\nText(0, 0.5, 'Count')\n\n\n\n\n\n\n\n\n\n\nCode\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    # In the dataset we only have float and int64.\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    # Categorical\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\nprint('There are {} numerical features in the dataset.'.format(len(numerical_features)))\n\n\nThere are 15 numerical features in the dataset.\n\n\n\n\n\n\n\nCode\nX_num_total = X[numerical_features]\nX_num_total.hist(bins=60,figsize=(20, 10))\nplt.show()\nprint('Number of uniques values of Accident Index: {}'.format(X_num_total['Accident_Index'].nunique()))\n\n\n\n\n\nNumber of uniques values of Accident Index: 75550\n\n\n\n\n\n\n\n\n\n\nCode\nprint('There are {} categorical features in the dataset.'.format(len(categorical_features)))\nX_cat_total = X[categorical_features]\nprint('Unique values for each categorical column are:\\n {}'.format(X_cat_total.nunique()))\n\n\nThere are 17 categorical features in the dataset.\nUnique values for each categorical column are:\n Region                        11\nUrban_or_Rural_Area            2\nX1st_Road_Class                6\nRoad_Type                      5\nRoad_Surface_Conditions        5\nWeather                        6\nHigh_Wind                      2\nLights                         4\nDatetime                   67926\nJunction_Detail                8\nJunction_Location              9\nX1st_Point_of_Impact           5\nDriver_Journey_Purpose         5\nPropulsion_Code                2\nVehicle_Make                  25\nVehicle_Category               6\nVehicle_Manoeuvre             11\ndtype: int64\n\n\n\n\nCode\ndata['Datetime']\n\n\n0        1/19/2010 17:30\n1         2/8/2010 11:24\n2          3/3/2010 6:25\n3         3/4/2010 13:35\n4        3/12/2010 16:05\n              ...       \n75545     3/6/2014 18:20\n75546    5/24/2014 15:50\n75547     9/8/2014 12:06\n75548    4/18/2014 15:52\n75549    8/27/2014 16:16\nName: Datetime, Length: 75550, dtype: object\n\n\n\n\n\n\n\nCode\nX = X.drop(columns=['Accident_Index','Datetime'])\n\n\n\n\n\n\n\nCode\nplt.subplots(figsize=(15,8))\nsns.heatmap(X_num_total.corr(), cmap=\"YlGnBu\", annot=True)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nX.to_csv('./cleandata/cleanUKgas.csv', index=False)\n\n\n\n\n\n\n\n\n\n\nCode\ndf = pd.read_csv('./Data/newapiTesla.csv')\ndef count_word_frequencies(data_frame, column_name, target_words):\n    text = ' '.join(data_frame[column_name])\n    words = text.split()\n    word_counts = {word: words.count(word) for word in set(target_words)}\n    return word_counts\n\ntarget_words = ['deadly', 'flawed', 'death', 'injury', 'casualty', 'accident', 'casualties', 'problem', 'bad', 'negative']\nword_frequencies = count_word_frequencies(df, 'title', target_words)\nfor word, count in word_frequencies.items():\n    print(f\"{word}: {count}\")\n\nplt.bar(word_frequencies.keys(), word_frequencies.values())\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequencies in title')\nplt.show()\n\n\nflawed: 1\ninjury: 0\ncasualties: 0\nproblem: 0\nbad: 0\ndeadly: 0\ncasualty: 0\naccident: 9\ndeath: 1\nnegative: 0\n\n\n\n\n\n\n\nCode\nword_frequencies = count_word_frequencies(df, 'content', target_words)\nfor word, count in word_frequencies.items():\n    print(f\"{word}: {count}\")\n\nplt.bar(word_frequencies.keys(), word_frequencies.values())\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequencies in content')\nplt.show()\n\n\nflawed: 0\ninjury: 0\ncasualties: 0\nproblem: 1\nbad: 2\ndeadly: 0\ncasualty: 0\naccident: 13\ndeath: 1\nnegative: 0"
  },
  {
    "objectID": "dataclean.html#first-dataset-tesla-deaths---deaths",
    "href": "dataclean.html#first-dataset-tesla-deaths---deaths",
    "title": "Data cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv(\"./Data/Tesla Deaths - Deaths.csv\")\n\n\n\n\n\n\nCode\nmsno.heatmap(df)\n\n\n\n\n\n\n\n\n\n\n\nCode\nnew_df = pd.DataFrame()\nfor i in range(len(df.columns[:14]),1,-1):\n    new_df.insert(0,df.columns[i],df[df.columns[i]])\ndf = new_df\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla driver\nTesla occupant\nOther vehicle\nCyclists/ Peds\nTSLA+cycl / peds\nModel\nAutopilot claimed\nVerified Tesla Autopilot Deaths\n\n\n\n\n0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n1\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n2\n1/7/2023\nUSA\nWA\nTesla hits pole, catches on fire\n1.0\n-\n1\n-\n-\n1\n-\n-\n-\n\n\n3\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n4\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n-\n-\n-\n1\n1\n-\n-\n-\n\n\n\n\n\n\n\n\n\nCode\nmsno.bar(df)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\nCode\nfor i in range(5,10):\n    df[df.columns[i]] = df[df.columns[i]].fillna(\"-\")\nfor i in range(11,13):\n    df[df.columns[i]] = df[df.columns[i]].fillna('-')\nmsno.bar(df)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\nCode\ndf = df.dropna()\nmsno.bar(df)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\ndf.columns = ['Date','Country','State','Description','Deaths',\"Tesla_driver\",\"Tesla_occupant\",\"Other_vehicle\",\"CP\",\"tsla+cp\",\"Model\",\"Claimed\",\"VTAD\"]\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla_driver\nTesla_occupant\nOther_vehicle\nCP\ntsla+cp\nModel\nClaimed\nVTAD\n\n\n\n\n0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n1\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n2\n1/7/2023\nUSA\nWA\nTesla hits pole, catches on fire\n1.0\n-\n1\n-\n-\n1\n-\n-\n-\n\n\n3\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n-\n-\n-\n1\n-\n-\n-\n\n\n4\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n-\n-\n-\n1\n1\n-\n-\n-"
  },
  {
    "objectID": "dataclean.html#second-data-uk-road-safety-traffic-accidents-and-vehicles-gas-car",
    "href": "dataclean.html#second-data-uk-road-safety-traffic-accidents-and-vehicles-gas-car",
    "title": "Data cleaning",
    "section": "",
    "text": "Code\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nprint(\"The dataset contains {} different data types and they are: {}\".format(len(column_datatypes), \", \".join(column_datatypes)))\nX = data.drop(columns='Accident_Severity')\ny = data['Accident_Severity']\ncount = pd.value_counts(y, sort = True)\ncount.plot(kind = 'bar', rot=0)\nplt.title(\"Distribution of Accident Severity \")\nplt.xlabel(\"Result\")\nplt.ylabel(\"Count\")\n\n\nThe dataset contains 3 different data types and they are: float64, object, int64\n\n\nText(0, 0.5, 'Count')\n\n\n\n\n\n\n\n\n\n\nCode\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    # In the dataset we only have float and int64.\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    # Categorical\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\nprint('There are {} numerical features in the dataset.'.format(len(numerical_features)))\n\n\nThere are 15 numerical features in the dataset.\n\n\n\n\n\n\n\nCode\nX_num_total = X[numerical_features]\nX_num_total.hist(bins=60,figsize=(20, 10))\nplt.show()\nprint('Number of uniques values of Accident Index: {}'.format(X_num_total['Accident_Index'].nunique()))\n\n\n\n\n\nNumber of uniques values of Accident Index: 75550\n\n\n\n\n\n\n\n\n\n\nCode\nprint('There are {} categorical features in the dataset.'.format(len(categorical_features)))\nX_cat_total = X[categorical_features]\nprint('Unique values for each categorical column are:\\n {}'.format(X_cat_total.nunique()))\n\n\nThere are 17 categorical features in the dataset.\nUnique values for each categorical column are:\n Region                        11\nUrban_or_Rural_Area            2\nX1st_Road_Class                6\nRoad_Type                      5\nRoad_Surface_Conditions        5\nWeather                        6\nHigh_Wind                      2\nLights                         4\nDatetime                   67926\nJunction_Detail                8\nJunction_Location              9\nX1st_Point_of_Impact           5\nDriver_Journey_Purpose         5\nPropulsion_Code                2\nVehicle_Make                  25\nVehicle_Category               6\nVehicle_Manoeuvre             11\ndtype: int64\n\n\n\n\nCode\ndata['Datetime']\n\n\n0        1/19/2010 17:30\n1         2/8/2010 11:24\n2          3/3/2010 6:25\n3         3/4/2010 13:35\n4        3/12/2010 16:05\n              ...       \n75545     3/6/2014 18:20\n75546    5/24/2014 15:50\n75547     9/8/2014 12:06\n75548    4/18/2014 15:52\n75549    8/27/2014 16:16\nName: Datetime, Length: 75550, dtype: object\n\n\n\n\n\n\n\nCode\nX = X.drop(columns=['Accident_Index','Datetime'])\n\n\n\n\n\n\n\nCode\nplt.subplots(figsize=(15,8))\nsns.heatmap(X_num_total.corr(), cmap=\"YlGnBu\", annot=True)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nX.to_csv('./cleandata/cleanUKgas.csv', index=False)"
  },
  {
    "objectID": "dataclean.html#third-dataset-acquired-using-news-api-and-focused-on-the-topic-of-tesla-related-accident",
    "href": "dataclean.html#third-dataset-acquired-using-news-api-and-focused-on-the-topic-of-tesla-related-accident",
    "title": "Data cleaning",
    "section": "",
    "text": "Code\ndf = pd.read_csv('./Data/newapiTesla.csv')\ndef count_word_frequencies(data_frame, column_name, target_words):\n    text = ' '.join(data_frame[column_name])\n    words = text.split()\n    word_counts = {word: words.count(word) for word in set(target_words)}\n    return word_counts\n\ntarget_words = ['deadly', 'flawed', 'death', 'injury', 'casualty', 'accident', 'casualties', 'problem', 'bad', 'negative']\nword_frequencies = count_word_frequencies(df, 'title', target_words)\nfor word, count in word_frequencies.items():\n    print(f\"{word}: {count}\")\n\nplt.bar(word_frequencies.keys(), word_frequencies.values())\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequencies in title')\nplt.show()\n\n\nflawed: 1\ninjury: 0\ncasualties: 0\nproblem: 0\nbad: 0\ndeadly: 0\ncasualty: 0\naccident: 9\ndeath: 1\nnegative: 0\n\n\n\n\n\n\n\nCode\nword_frequencies = count_word_frequencies(df, 'content', target_words)\nfor word, count in word_frequencies.items():\n    print(f\"{word}: {count}\")\n\nplt.bar(word_frequencies.keys(), word_frequencies.values())\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequencies in content')\nplt.show()\n\n\nflawed: 0\ninjury: 0\ncasualties: 0\nproblem: 1\nbad: 2\ndeadly: 0\ncasualty: 0\naccident: 13\ndeath: 1\nnegative: 0"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a method used in machine learning and data analysis to group similar data points together based on certain features or characteristics. The goal of clustering is to partition a dataset into subsets, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. This helps in identifying patterns, structures, or hidden relationships within the data.\n\n\n\nIn this analysis, the feature data X pertains to the state the accident took place, while the Y variables include ‘CP,’ ‘tsla+cp,’ ‘VTAD,’ ‘Claimed,’ ‘Tesla_occupant,’ and ‘Other_vehicle.’ Notably, conventional visualization techniques like PCA and t-SNE prove ineffective due to the absence of a distinct separation between clusters. Consequently, clustering methods such as K-means are employed in this section for a more suitable exploration of the data’s inherent patterns.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n\n\n\n\n\ndf = pd.read_csv('./cleandata/cleanTelsa.csv')\nfeatures = df[['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle']]\ntarget = df['State']\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    km = KMeans(n_clusters=k, random_state=8)\n    km.fit(features_scaled)\n    inertias.append(km.inertia_)\n    \nplt.plot(ks, inertias, marker='o')\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\np_comps = pca.fit_transform(features_scaled)\np_comp1 = p_comps[:,0]\np_comp2 = p_comps[:,1]\nkm = KMeans(n_clusters=3,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x23ffcbd8940&gt;\n\n\n\n\n\n\n\n\nIt appears that there isn’t a substantial difference between utilizing k=3 and k=4. Therefore, for the sake of diversity, we will adhere to using k=4.\n\nkm = KMeans(n_clusters=4,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x23ffd659ff0&gt;\n\n\n\n\n\n\n\n\nA logical progression in our analysis involves examining the distinctions among the three clusters concerning the three features employed for clustering. Instead of relying on scaled features, we revert to using the unscaled features to facilitate a more interpretable exploration of the differences.\n\n\n\n\nimport seaborn as sns\ndf['cluster'] = km.labels_\nmelt_car = pd.melt(df,id_vars='cluster',var_name=\"predictor\",value_name=\"percent\",\n                 value_vars=['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle'] )\nsns.violinplot(data=melt_car,y='predictor',x='percent',hue='cluster')\n\n&lt;Axes: xlabel='percent', ylabel='predictor'&gt;\n\n\n\n\n\n\n\n\nFrom the presented plot, it is now evident that distinct clusters of states may necessitate varied interventions, such as implementing safety measures like road cushioning or enforcing more stringent traffic laws tailored to each group’s specific characteristics."
  },
  {
    "objectID": "clustering.html#clustering-on-the-dataset-tesla-deaths---deaths",
    "href": "clustering.html#clustering-on-the-dataset-tesla-deaths---deaths",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a method used in machine learning and data analysis to group similar data points together based on certain features or characteristics. The goal of clustering is to partition a dataset into subsets, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. This helps in identifying patterns, structures, or hidden relationships within the data.\n\n\n\nIn this analysis, the feature data X pertains to the state the accident took place, while the Y variables include ‘CP,’ ‘tsla+cp,’ ‘VTAD,’ ‘Claimed,’ ‘Tesla_occupant,’ and ‘Other_vehicle.’ Notably, conventional visualization techniques like PCA and t-SNE prove ineffective due to the absence of a distinct separation between clusters. Consequently, clustering methods such as K-means are employed in this section for a more suitable exploration of the data’s inherent patterns.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n\n\n\n\n\ndf = pd.read_csv('./cleandata/cleanTelsa.csv')\nfeatures = df[['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle']]\ntarget = df['State']\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    km = KMeans(n_clusters=k, random_state=8)\n    km.fit(features_scaled)\n    inertias.append(km.inertia_)\n    \nplt.plot(ks, inertias, marker='o')\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\np_comps = pca.fit_transform(features_scaled)\np_comp1 = p_comps[:,0]\np_comp2 = p_comps[:,1]\nkm = KMeans(n_clusters=3,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x23ffcbd8940&gt;\n\n\n\n\n\n\n\n\nIt appears that there isn’t a substantial difference between utilizing k=3 and k=4. Therefore, for the sake of diversity, we will adhere to using k=4.\n\nkm = KMeans(n_clusters=4,random_state=10)\nkm.fit(features_scaled)\nplt.scatter(p_comps[:,0],p_comps[:,1],c=km.labels_)\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x23ffd659ff0&gt;\n\n\n\n\n\n\n\n\nA logical progression in our analysis involves examining the distinctions among the three clusters concerning the three features employed for clustering. Instead of relying on scaled features, we revert to using the unscaled features to facilitate a more interpretable exploration of the differences.\n\n\n\n\nimport seaborn as sns\ndf['cluster'] = km.labels_\nmelt_car = pd.melt(df,id_vars='cluster',var_name=\"predictor\",value_name=\"percent\",\n                 value_vars=['CP','tsla+cp', 'VTAD', 'Claimed', 'Tesla_occupant','Other_vehicle'] )\nsns.violinplot(data=melt_car,y='predictor',x='percent',hue='cluster')\n\n&lt;Axes: xlabel='percent', ylabel='predictor'&gt;\n\n\n\n\n\n\n\n\nFrom the presented plot, it is now evident that distinct clusters of states may necessitate varied interventions, such as implementing safety measures like road cushioning or enforcing more stringent traffic laws tailored to each group’s specific characteristics."
  },
  {
    "objectID": "clustering.html#clustering-on-the-dataset-traffic-accidents-and-vehicles-gas-car",
    "href": "clustering.html#clustering-on-the-dataset-traffic-accidents-and-vehicles-gas-car",
    "title": "Clustering",
    "section": "Clustering on the dataset: Traffic Accidents and Vehicles (gas car)",
    "text": "Clustering on the dataset: Traffic Accidents and Vehicles (gas car)\n\n\nCode\ndata = pd.read_csv('./Data/RoadAccident.csv')\ncolumn_datatypes = set()\nfor column in data.columns:\n    column_datatypes.add(str(data[column].dtype))\nX = data.drop(columns='Accident_Severity')\ny = data['Accident_Severity']\nnumerical_features = list()\ncategorical_features = list()\nfor column in X.columns:\n    # In the dataset we only have float and int64.\n    if (data[column].dtype == 'float64' or data[column].dtype == 'int64'):\n        numerical_features.append(column)\n    # Categorical\n    elif (data[column].dtype == 'object'):\n        categorical_features.append(column)\ndata = X[numerical_features]\nSS=StandardScaler()\nX=pd.DataFrame(SS.fit_transform(data), columns=data.columns)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nprincipal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\nkmeans = KMeans(n_clusters=5, n_init=15, max_iter=500, random_state=0)\nclusters = kmeans.fit_predict(X)\ncentroids = kmeans.cluster_centers_\ncentroids_pca = pca.transform(centroids)\n\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\base.py:464: UserWarning:\n\nX does not have valid feature names, but PCA was fitted with feature names\n\n\n\n\nPlot the clustering results both in 2-D and 3-D plot\n\nplt.figure(figsize=(8,6))\nplt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], c=clusters, cmap=\"brg\", s=40)\nplt.scatter(x=centroids_pca[:,0], y=centroids_pca[:,1], marker=\"x\", s=100, linewidths=3, color=\"black\")\nplt.title('PCA plot in 2D')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\nText(0, 0.5, 'PC2')\n\n\n\n\n\n\npca = PCA(n_components=3)\ncomponents = pca.fit_transform(X)\nimport plotly.express as px\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=clusters, size=0.1*np.ones(len(X)), opacity = 1,\n    title='PCA plot in 3D',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    width=650, height=500\n)\nfig.show()\n\n\n                                                \n\n\n\n\nIn summary:\n\nClustering is not distinctly visible in both 2-D and 3-D plots, primarily due to the extensive size of the dataset, leading to overlapping scatter plots. However, subsetting the dataset would improve visualization clarity.\nUtilizing clustering methods enables the categorization of the entire dataset into smaller groups, each representing distinct types of accidents. Subsequently, any of these groups can be selected for in-depth analysis."
  },
  {
    "objectID": "aboutyou.html#backgrounds",
    "href": "aboutyou.html#backgrounds",
    "title": "About Me",
    "section": "Backgrounds:",
    "text": "Backgrounds:\nHey everyone, I am Zenan Wang, commonly known as Will, hailing from Nanjing, China. I completed my undergraduate studies at UNC Chapel Hill, achieving a double major in Mathematics and Statistics. My interests include cycling, fitness activities, and indulging in video games for leisure.\n\nLinkedin link: https://www.linkedin.com/in/zenan-wang-062695260/"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "",
    "section": "",
    "text": "Link to code: https://github.com/anly501/dsan-5000-project-WillWangUNC"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "",
    "section": "",
    "text": "link to data: https://github.com/anly501/dsan-5000-project-WillWangUNC/tree/main/dsan-website/5000-website/5000-website-Zenan/Data"
  },
  {
    "objectID": "dataexplore.html",
    "href": "dataexplore.html",
    "title": "Data exploring",
    "section": "",
    "text": "EDA, or Exploratory Data Analysis, is a crucial phase in the data analysis process that involves summarizing the main characteristics of a dataset, often with the help of statistical graphics and other data visualization methods. The primary goal of EDA is to understand the structure and key features of the data, discover patterns, identify potential outliers, and generate hypotheses for further analysis.\n\n\n\nTesla Deaths - Deaths\nUK Road Safety: Traffic Accidents and Vehicles (gas car)"
  },
  {
    "objectID": "dataexplore.html#introduction",
    "href": "dataexplore.html#introduction",
    "title": "Data exploring",
    "section": "",
    "text": "EDA, or Exploratory Data Analysis, is a crucial phase in the data analysis process that involves summarizing the main characteristics of a dataset, often with the help of statistical graphics and other data visualization methods. The primary goal of EDA is to understand the structure and key features of the data, discover patterns, identify potential outliers, and generate hypotheses for further analysis.\n\n\n\nTesla Deaths - Deaths\nUK Road Safety: Traffic Accidents and Vehicles (gas car)"
  },
  {
    "objectID": "dataexplore.html#first-dataset-tesla-deaths---deaths",
    "href": "dataexplore.html#first-dataset-tesla-deaths---deaths",
    "title": "Data exploring",
    "section": "First dataset: Tesla Deaths - Deaths",
    "text": "First dataset: Tesla Deaths - Deaths\n\nPre-process the data and take a look at the cleaned data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\ndf = pd.read_csv(\"./Data/Tesla Deaths - Deaths.csv\")\nnew_df = pd.DataFrame()\nfor i in range(len(df.columns[:14]),1,-1):\n    new_df.insert(0,df.columns[i],df[df.columns[i]])\ndf = new_df\nfor i in range(5,10):\n    df[df.columns[i]] = df[df.columns[i]].fillna(\"-\")\n    \nfor i in range(11,13):\n    df[df.columns[i]] = df[df.columns[i]].fillna('-')\ndf = df.dropna()\ndf.columns = ['Date','Country','State','Description','Deaths',\"Tesla_driver\",\"Tesla_occupant\",\"Other_vehicle\",\"CP\",\"tsla+cp\",\"Model\",\"Claimed\",\"VTAD\"]\nfor i in range(5,13):\n    for b in range(len(df)):\n        if \"-\"in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 0\n        elif \"1\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 1\n        elif \"2\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 2\n        elif \"3\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 3\n        elif \"4\" in df[df.columns[i]].values[b]:\n            df[df.columns[i]].values[b] = 4\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\23898\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n\n\nCode\ndf.head()\ndf.to_csv('./cleandata/cleanTelsa.csv', index=False)"
  },
  {
    "objectID": "dataexplore.html#start-eda",
    "href": "dataexplore.html#start-eda",
    "title": "Data exploring",
    "section": "Start EDA",
    "text": "Start EDA\n\nThe ‘Date’ variable is not in the default date format\n\n\nCode\ndf.loc[:, \"event_year\"] = 0\ndf.loc[:, \"event_month\"] = 0\ndf.loc[:, \"event_day\"] = 0\nfor i in range(len(df)):\n    df.loc[df.index[i], \"event_year\"] = int(df[\"Date\"].values[i].split('/')[2])\n    df.loc[df.index[i], \"event_month\"] = int(df[\"Date\"].values[i].split('/')[0])\n    df.loc[df.index[i], \"event_day\"] = int(df[\"Date\"].values[i].split('/')[1])\ndf['Date']\n\n\n0       1/17/2023\n1        1/7/2023\n2        1/7/2023\n3      12/22/2022\n4      12/19/2022\n          ...    \n289     7/14/2014\n290      7/4/2014\n291      7/4/2014\n292     11/2/2013\n293      4/2/2013\nName: Date, Length: 294, dtype: object\n\n\n\n\nLet’s see the distribution of crashes around the world\nIt seems that USA has the most accidents, then followed by China and Germany.\n\n\nCode\nx = df[\"Country\"].value_counts().index\ny = df[\"Country\"].value_counts().values\nplt.figure(figsize=(20,8))\nfor i in range(len(x)):\n    height = y[i]\n    plt.text(x[i], height + 0.25, '%.1f' %height, ha='center', va='bottom', size = 12)\nplt.bar(x,y,color='#e35f62')\n\n\n&lt;BarContainer object of 23 artists&gt;\n\n\n\n\n\n\n\nLet’s look at the number of accidents per month\n\n\nCode\nplt.figure(figsize=(20,8))\nx = df[\"event_month\"].value_counts().sort_index().index\ny = df[\"event_month\"].value_counts().sort_index().values\nfor i in range(len(x)):\n    height = y[i]\n    plt.text(x[i], height + 0.5, '%.1f' %height, ha='center', va='bottom', size = 12)\nplt.title(\"Number of accidents per month in the total year\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Number of events\")\nplt.bar(x,y)\n\n\n&lt;BarContainer object of 12 artists&gt;\n\n\n\n\n\n\n\nIt appears that there are relatively more accidents in November and December.\n\n\nLet’s look at the distribution of the following variables:\n\n\nCode\nd_list = [\"Deaths\",\"Tesla_driver\",\"Tesla_occupant\",\"CP\",\"tsla+cp\",\"Other_vehicle\"]\nlabel = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\nplt.figure(figsize = (16,16))\nfor b in range(len(d_list)):\n    size = df[d_list[b]].value_counts().values\n    colors = []\n    label = df[d_list[b]].value_counts().index\n    plt.axis(\"equal\")\n    plt.rc(\"font\",family=\"Malgun Gothic\")\n    plt.rc('legend', fontsize=10)\n    plt.subplot(2,3,b+1)\n    plt.title(d_list[b]+\" - total:\" +str(len(df[d_list[b]])))\n    plt.pie(size,labels=label, autopct = \"%.1f%%\")\n    plt.legend()\n\n\n\n\n\n\n\nThe above collection of pie charts indicates that the majority of the accidents involve one casualty.\n\n\nwhy are there more accidents in November and December? Does that necessarily mean Tesla is maneuverable?\nSince there is a ‘description’ variable in the dataset, which briefly depicts the possible cause and the crash scene, I will classify the accidents into smaller categories.\n\n\nCode\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1]\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef lemmatize_text(text):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(text)]\ndata = df[\"Description\"]\n\nlemmatized_data = [lemmatize_text(text) for text in data]\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform([' '.join(text) for text in lemmatized_data])\n\nkmeans = KMeans(n_clusters= 5, random_state=0)\nkmeans.fit(X)\nclusters = kmeans.predict(X)\n\n\nC:\\Users\\23898\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\nLet’s see how the classification goes.\n\n\nCode\ndf[\"Description\"].value_counts()\n\n\nDescription\n Tesla kills pedestrian                         10\n Tesla kills motorcyclist                        6\n Tesla into oncoming traffic                     5\n Tesla rear ends stopped car                     4\n Tesla drives off cliff                          4\n                                                ..\n Tesla crashes into trees                        1\n Multi-crash involving DUI                       1\n Tesla loses control and drives into river       1\n Crash in public area including 20 injuries      1\n Tesla veers into opposite lane                  1\nName: count, Length: 248, dtype: int64\n\n\n\n\nLet’s look at the distribution of the classification\n\n\nCode\ndf.loc[:, \"cluster\"] = 0\nfor i, item in enumerate(df[\"Description\"]):\n    for cluster, description in zip(clusters, data):\n        if item == description:\n            df[\"cluster\"][i] = cluster\naccident_0 = []\naccident_1 = []\naccident_2 = []\naccident_3 = []\naccident_4 = []\nfor i, cluster in enumerate(clusters):\n    if cluster == 0:\n        accident_0.append(data[i])\n    elif cluster == 1:\n        accident_1.append(data[i])\n    elif cluster == 2:\n        accident_2.append(data[i])\n    elif cluster == 3:\n        accident_3.append(data[i])\n    elif cluster == 4:\n        accident_4.append(data[i])\naccident_0_score = []\naccident_1_score = []\naccident_2_score = []\naccident_3_score = []\naccident_4_score = []\n\ndef get_score_list(a,b1):\n    for i in range(len(a)):\n        for b in range(len(df[\"Description\"].value_counts().index)):\n            if df[\"Description\"].value_counts().index[b] == a[i]:\n                b1.append(df[\"Description\"].value_counts().values[b])\n                \nget_score_list(accident_0,accident_0_score)\nget_score_list(accident_1,accident_1_score)\nget_score_list(accident_2,accident_2_score)\nget_score_list(accident_3,accident_3_score)\nget_score_list(accident_4,accident_4_score)\nx = [0,1,2,3,4]\ny = [len(accident_0),len(accident_1),len(accident_2),len(accident_3),len(accident_4)]\nplt.figure(figsize=(20,10))\n# plt.title(\"accident type's counts\")\nplt.xlabel(\"accident type\")\nplt.ylabel(\"counts of accident type\")\nfor i in range(len(x)):\n    height = y[i]\n    plt.text(x[i], height + 0.25, '%.1f' %height, ha='center', va='bottom', size = 12)\nplt.bar(x,y,color='red')\n\n\n&lt;BarContainer object of 5 artists&gt;\n\n\n\n\n\n\n\nLet’s see what type-2 accident is about.\nIt seems that this type of collision has little to do with the maneuverability of Tesla.\n\n\nCode\ndf[df['cluster'] == 2].head(10)\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla_driver\nTesla_occupant\nOther_vehicle\nCP\ntsla+cp\nModel\nClaimed\nVTAD\nevent_year\nevent_month\nevent_day\ncluster\n\n\n\n\n8\n12/11/2022\nUSA\nMO\nCollision at intersection\n1.0\n0\n0\n1\n0\n0\n0\n0\n0\n2022\n12\n11\n2\n\n\n9\n12/6/2022\nCanada\n-\nTesla veers, collides with truck\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n12\n6\n2\n\n\n10\n11/28/2022\nChina\n-\nTesla runs red light, collides with two cars\n2.0\n0\n0\n2\n0\n0\nY\n0\n0\n2022\n11\n28\n2\n\n\n15\n11/12/2022\nUSA\nCA\nMulti-vehicle accident\n1.0\n0\n0\n1\n0\n0\n0\n0\n0\n2022\n11\n12\n2\n\n\n18\n11/4/2022\nUSA\nIL\nCollision at intersection, Tesla driver dies ...\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n11\n4\n2\n\n\n20\n10/18/2022\nUSA\nFL\nTesla collides with minivan, engulfed by flames\n4.0\n1\n1\n2\n0\n2\n0\n0\n0\n2022\n10\n18\n2\n\n\n26\n9/16/2022\nUSA\nGA\nTesla loses control and crashes into bus shel...\n1.0\n0\n0\n0\n1\n1\n0\n1\n0\n2022\n9\n16\n2\n\n\n27\n9/13/2022\nUSA\nCA\nTesla runs off highway\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n9\n13\n2\n\n\n28\n9/12/2022\nUSA\nNY\nTesla runs off road, catches fire\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n9\n12\n2\n\n\n29\n9/7/2022\nUSA\nCA\nMotorcycle collides with Tesla\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n9\n7\n2\n\n\n\n\n\n\n\n\n\nLet’s see what type-0 accident is about.\nIt seems that this type of collision also has little to do with the maneuverability of Tesla.\n\n\nCode\ndf[df['cluster'] == 0].head(10)\n\n\n\n\n\n\n\n\n\nDate\nCountry\nState\nDescription\nDeaths\nTesla_driver\nTesla_occupant\nOther_vehicle\nCP\ntsla+cp\nModel\nClaimed\nVTAD\nevent_year\nevent_month\nevent_day\ncluster\n\n\n\n\n0\n1/17/2023\nUSA\nCA\nTesla crashes into back of semi\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2023\n1\n17\n0\n\n\n1\n1/7/2023\nCanada\n-\nTesla crashes\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2023\n1\n7\n0\n\n\n3\n12/22/2022\nUSA\nGA\nTesla crashes and burns\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n12\n22\n0\n\n\n4\n12/19/2022\nCanada\n-\nTesla crashes into storefront\n1.0\n0\n0\n0\n1\n1\n0\n0\n0\n2022\n12\n19\n0\n\n\n7\n12/11/2022\nUSA\nCA\nTesla crashes into wall\n1.0\n1\n0\n0\n0\n0\n0\n0\n0\n2022\n12\n11\n0\n\n\n13\n11/18/2022\nChina\n-\nTesla crashes into dump truck\n1.0\n1\n0\n0\n0\n1\n0\n0\n0\n2022\n11\n18\n0\n\n\n19\n10/19/2022\nUSA\nCA\nMulticar crash\n1.0\n1\n0\n0\n0\n1\n1\n0\n0\n2022\n10\n19\n0\n\n\n21\n10/12/2022\nUK\n-\nTesla crashes into ditch\n1.0\n1\n0\n0\n0\n1\n2\n0\n0\n2022\n10\n12\n0\n\n\n24\n9/18/2022\nUSA\nSC\nTesla crashes into tree, ignites\n2.0\n1\n1\n0\n0\n2\n0\n0\n0\n2022\n9\n18\n0\n\n\n25\n9/18/2022\nUSA\nMD\nMulti-car accident\n1.0\n0\n0\n1\n0\n0\n0\n0\n0\n2022\n9\n18\n0\n\n\n\n\n\n\n\n\n\nNumber of deaths in each type of accident\n\n\nCode\nfor i in range(0,5):\n    df[df[\"cluster\"]==i][\"Deaths\"].value_counts().sort_index().plot(figsize=(20,8))\n    plt.xlabel(\"total Death\")\n    plt.ylabel(\"ac type Counts\")\n    plt.title(\"ac type Counts - Death\")\n    plt.legend(\"01234\")\n\n\n\n\n\n\n\nDistribution of each type of accident at each year recorded\n\n\nCode\ndef make_Graph(feature):\n    x_s = df[feature].value_counts().sort_index().index\n    y_s = df[feature].value_counts().sort_index().values\n    y0_s = df[df[\"cluster\"]==0][feature].value_counts().sort_index()\n    y1_s = df[df[\"cluster\"]==1][feature].value_counts().sort_index()\n    y2_s = df[df[\"cluster\"]==2][feature].value_counts().sort_index()\n    y3_s = df[df[\"cluster\"]==3][feature].value_counts().sort_index()\n    y4_s = df[df[\"cluster\"]==4][feature].value_counts().sort_index()\n    def fill_y(x,y):\n        for i in x:\n            if i not in y.index:\n                y[i] = 0\n    fill_y(x_s,y0_s)\n    fill_y(x_s,y1_s)\n    fill_y(x_s,y2_s)\n    fill_y(x_s,y3_s)\n    fill_y(x_s,y4_s)\n    \n#     plt.figure(figsize=(10,8))\n    ax,ax0,ax1,ax2,ax3,ax4 = plt.gca(),plt.gca(),plt.gca(),plt.gca(),plt.gca(),plt.gca()\n\n    for i in range(len(x_s)):\n        height = y_s[i]\n        plt.text(x_s[i], height + 0.15, '%.1f' %height, ha='center', va='bottom', size = 12)\n\n    ax.bar(x_s, y_s, color='orange', linestyle='--')\n    ax.set_ylabel(feature+' - Counts', fontsize=10)\n    ax.tick_params('y', colors='blue') \n\n    ax0.plot(x_s, y0_s.sort_index().values, color='yellow', linestyle='--',label = \"0\")\n    ax1.plot(x_s, y1_s.sort_index().values, color='blue', linestyle='--',label = \"1\")\n    ax2.plot(x_s, y2_s.sort_index().values, color='red', linestyle='--',label = \"2\")\n    ax3.plot(x_s, y3_s.sort_index().values, color='pink', linestyle='--',label = \"3\")\n    ax4.plot(x_s, y4_s.sort_index().values, color='green', linestyle='--',label = \"4\")\n\n    ax1.set_xlabel(feature, fontsize=10)\n    plt.title(feature+\" Accident counts & Accident Type counts\",fontsize=13)\n    plt.tight_layout()\n    plt.legend()\n    plt.figure(figsize=(20,8))\nmake_Graph(\"event_year\")\n\n\n\n\n\n&lt;Figure size 1920x768 with 0 Axes&gt;\n\n\n\n\nConclusion:\nWe do not have enough evidence to claim that the maneuverability of Tesla is flawed, since most accidents are caused by human errors. However, more accidents did occur at latest years, which might be due to the largely increased number of Tesla on road."
  },
  {
    "objectID": "dataexplore.html#second-dataset-uk-road-safety-traffic-accidents-and-vehicles-gas-car",
    "href": "dataexplore.html#second-dataset-uk-road-safety-traffic-accidents-and-vehicles-gas-car",
    "title": "Data exploring",
    "section": "Second dataset: UK Road Safety: Traffic Accidents and Vehicles (gas car)",
    "text": "Second dataset: UK Road Safety: Traffic Accidents and Vehicles (gas car)\n\n\nCode\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.pipeline import Pipeline\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import boxcox\nimport pickle\nfrom numpy.random import seed\nseed(862)\nfrom tensorflow.random import set_seed\nset_seed(862)\ndata = pd.read_csv(\"./Data/RoadAccident.csv\")\ndata.drop(['Accident_Index','Datetime'], axis = 1,inplace=True) \nd = {\"Feature\":[i for i in data.columns]    ,\"Nunique\" :data.nunique().values ,'Type' : data.dtypes.values, \"No: of nulls\" : data.isnull().sum() }\ndescription = pd.DataFrame(data = d)\ndescription\ndata[\"Season\"]=data[\"Season\"].astype(str)\ndata[\"Month_of_Year\"]=data[\"Month_of_Year\"].astype(str)\ndata[\"Day_of_Week\"]=data[\"Day_of_Week\"].astype(str)\ndata[\"Year\"]=data[\"Year\"].astype(str)\ndata[\"Number_of_Vehicles\"]=data[\"Number_of_Vehicles\"].astype(str)\nCounter(data['Accident_Severity'])\n\n\nCounter({'Slight': 56705, 'Fatal_Serious': 18845})\n\n\n\nPlot count plots for the categorical data and histogram for numerical data\n\n\nCode\ncat_data  = data.select_dtypes(exclude=[np.number])\nfor i in cat_data:  \n  plt.figure(figsize=(10,10))# Creating an empty plot \n  ax=sns.countplot(x=cat_data[i],hue=data[\"Accident_Severity\"])# Countplot of airlines\n  plt.tick_params(labelsize=10)# changing the label sizes  \n  plt.ylabel(\"Count\" ,fontsize=10) #Adding y-label\n  #plt.title(\"\\n\", cat_data.columns.values,\"\\n\",fontsize=25) # Adding plot title\n  for p in ax.patches:\n      ax.annotate('{}'.format(p.get_height()),(p.get_x()+0.25,p.get_height()+5)) # Adding the count above the bars\n  plt.show() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at the distribution of all numerical variables\n\n\nCode\ndef diagnostic_plots(df, variable):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    df[variable].hist()\n    print(str(variable))\n    plt.subplot(1, 2, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.show()\nnum_data  = data.select_dtypes(include=[np.number])\nfor i in num_data:\n    diagnostic_plots(num_data, i) \n\n\nLatitude\nLongitude\nDriver_IMD_Decile\nSpeed_limit\nDay_of_Month\nHour_of_Day\nAge_of_Driver\nAge_of_Vehicle\nEngine_CC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of the above distribution plots:\n\nFor some categorical variable, there are many ranks that have very low frequency. For example, only the ‘fine’ rank of the ‘Weather’ variable has significant count, hence the other ranks might be grouped into one for easy processing.\nFor some numerical variables, some features are not normally distributed. This would be taken care of in future analysis.\n\n\n\nLook at the heatmap of the numerical variables\n\n\nCode\ncorrmat = data.select_dtypes(include=[np.number]).corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(14,6))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n\n\n\n\n\n\n\nThe above heatmap does not indicate any unusual correlation."
  },
  {
    "objectID": "dataexplore.html#tools-and-software",
    "href": "dataexplore.html#tools-and-software",
    "title": "Data exploring",
    "section": "Tools and software",
    "text": "Tools and software\nThe creation of this tab primarily involves Python, utilizing essential libraries such as Pandas, Missingno, NLTK, and Seaborn."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "The competition between electric cars and gas cars",
    "section": "",
    "text": "link: https://dash.harvard.edu/bitstream/handle/1/33826493/CORNELL-DOCUMENT-2017.pdf?isAllowed=y&sequence=1"
  },
  {
    "objectID": "introduction.html#first-publication-the-environmental-benefits-of-electric-vehicles-as-a-function-of-renewable-energy",
    "href": "introduction.html#first-publication-the-environmental-benefits-of-electric-vehicles-as-a-function-of-renewable-energy",
    "title": "The competition between electric cars and gas cars",
    "section": "",
    "text": "link: https://dash.harvard.edu/bitstream/handle/1/33826493/CORNELL-DOCUMENT-2017.pdf?isAllowed=y&sequence=1"
  },
  {
    "objectID": "introduction.html#second-publication-insights-for-more-reliable-electric-vehicles",
    "href": "introduction.html#second-publication-insights-for-more-reliable-electric-vehicles",
    "title": "The competition between electric cars and gas cars",
    "section": "Second publication: Insights for More Reliable Electric Vehicles",
    "text": "Second publication: Insights for More Reliable Electric Vehicles\nlink: https://data.consumerreports.org/wp-content/uploads/2022/01/Consumer-Reports-Insights-for-More-Reliable-Electric-Vehicles-Jan-2022.pdf"
  },
  {
    "objectID": "introduction.html#the-competition-between-electric-vehicles-evs-and-traditional-gasoline-powered-cars-is-a-complex-and-dynamic-topic-that-encompasses-various-dimensions-such-as-technology-economics-environmental-impact-and-consumer-preferences.-here-is-a-summarized-overview-along-with-insights-into-future-trends-and-diverse-perspectives-found-in-the-literature",
    "href": "introduction.html#the-competition-between-electric-vehicles-evs-and-traditional-gasoline-powered-cars-is-a-complex-and-dynamic-topic-that-encompasses-various-dimensions-such-as-technology-economics-environmental-impact-and-consumer-preferences.-here-is-a-summarized-overview-along-with-insights-into-future-trends-and-diverse-perspectives-found-in-the-literature",
    "title": "The competition between electric cars and gas cars",
    "section": "The competition between electric vehicles (EVs) and traditional gasoline-powered cars is a complex and dynamic topic that encompasses various dimensions such as technology, economics, environmental impact, and consumer preferences. Here is a summarized overview along with insights into future trends and diverse perspectives found in the literature:",
    "text": "The competition between electric vehicles (EVs) and traditional gasoline-powered cars is a complex and dynamic topic that encompasses various dimensions such as technology, economics, environmental impact, and consumer preferences. Here is a summarized overview along with insights into future trends and diverse perspectives found in the literature:\n\nEnvironmental Concerns:\n\nPro-EV Perspective: Electric vehicles are often considered more environmentally friendly due to their lower or zero emissions during operation.\nPro-Gas Car Perspective: Critics argue that the overall environmental impact of EVs depends on factors like electricity sources and battery production, which may involve resource-intensive processes.\n\n\n\nTechnological Advancements:\n\nPro-EV Perspective: Rapid advancements in battery technology are increasing the range and reducing the cost of electric vehicles, making them more competitive and appealing to consumers.\nPro-Gas Car Perspective: Traditional vehicles are benefiting from advancements in internal combustion engines and alternative fuels, maintaining their relevance and efficiency.\n\n\n\nInfrastructure and Range Anxiety:\n\nPro-EV Perspective: The development of charging infrastructure and improvements in battery technology are mitigating range anxiety, making EVs more practical for everyday use.\nPro-Gas Car Perspective: Gasoline vehicles have a well-established refueling infrastructure, providing convenience and ease of use without the need for extensive charging networks.\n\n\n\nEconomic Factors:\n\nPro-EV Perspective: Over time, as battery costs decrease and economies of scale are achieved, electric vehicles are expected to become more affordable, potentially reaching price parity with traditional cars.\nPro-Gas Car Perspective: Gasoline vehicles currently have a cost advantage in terms of manufacturing and maintenance, and the existing infrastructure supports their continued dominance.\n\n\n\nConsumer Adoption and Preferences:\n\nPro-EV Perspective: Growing awareness of environmental issues, government incentives, and changing consumer preferences are driving increased adoption of electric vehicles.\nPro-Gas Car Perspective: Consumer habits and perceptions, including concerns about charging infrastructure, vehicle range, and the initial cost of EVs, may slow down widespread adoption."
  },
  {
    "objectID": "introduction.html#future-trends-and-different-points-of-views",
    "href": "introduction.html#future-trends-and-different-points-of-views",
    "title": "The competition between electric cars and gas cars",
    "section": "Future Trends and Different Points of Views:",
    "text": "Future Trends and Different Points of Views:\n\nMarket Share Predictions:\n\nOptimistic View: Some experts predict a significant increase in EV market share, expecting a transition to an all-electric future as technology improves and consumers become more environmentally conscious.\nCautious View: Skeptics argue that gasoline vehicles will maintain a substantial market share for an extended period, as challenges such as battery production limitations and charging infrastructure development persist.\n\n\n\nGovernment Policies:\n\nSupportive Policies: Governments worldwide are implementing policies to encourage EV adoption, including subsidies, tax incentives, and regulations promoting cleaner transportation.\nMarket-Led Transition: Some argue that market forces, rather than government intervention, should guide the transition to electric vehicles, allowing for a more organic and sustainable shift.\n\n\n\nEnergy Source Diversification:\n\nRenewable Energy Integration: A positive outlook emphasizes the integration of renewable energy sources into the electricity grid, enhancing the environmental benefits of electric vehicles.\nEnergy Source Dependence: Critics express concerns about the environmental impact of increased electricity demand, especially if it relies on non-renewable energy sources.\n\n\n\nInnovation and Disruption:\n\nDisruptive Innovation: Proponents of EVs argue that they represent a disruptive innovation that will reshape the automotive industry, with new entrants and technologies challenging the status quo.\nIncremental Change: Skeptics suggest that the automotive industry will evolve gradually, with improvements in internal combustion engine efficiency and the coexistence of various propulsion technologies.\n\n\n\nIn conclusion, the competition between electric and gasoline vehicles is a multifaceted issue, with ongoing debates about the environmental impact, technological advancements, infrastructure development, economic factors, and consumer preferences. The future trajectory will likely involve a combination of market-driven forces, government policies, and technological breakthroughs, and diverse perspectives will continue to shape the discourse in this rapidly evolving landscape."
  },
  {
    "objectID": "introduction.html#questions-i-would-to-answer-to-better-explore-my-topic",
    "href": "introduction.html#questions-i-would-to-answer-to-better-explore-my-topic",
    "title": "The competition between electric cars and gas cars",
    "section": "10 Questions I would to answer to better explore my topic:",
    "text": "10 Questions I would to answer to better explore my topic:\n\nSafety:\nQuestion: Is an electric vehicle (EV) less controllable and more prone to casualties in the event of a crash?\n\n\nAccident severity:\nQuestion: Is an accident involving an electric vehicle (EV) more likely to result in minor or severe outcomes?\n\n\nPredict the accident outcome:\nQuestion: Is there a model available for predicting accident outcomes irrespective of whether the vehicle is electric or gasoline-powered?\n\n\nThe cause of the frequent accidents of EVs and solution:\nQuestion: What might account for the frequent accidents of EVs?\n\n\nBattery Technology Analysis:\nQuestion: What advancements in battery technology have had the most significant impact on the performance and cost-effectiveness of electric vehicles, and how do these innovations compare with improvements in internal combustion engine technology?\n\n\nGovernment Policy Impact:\nQuestion: How do government policies, such as subsidies, incentives, and regulations, influence the market share and growth of electric vehicles? Can we identify policy measures that have been particularly effective?\n\n\nEnergy Source Analysis:\nQuestion: What is the current and projected mix of energy sources used to generate electricity for electric vehicles, and how does this impact the overall environmental benefits of EVs in different regions? Consumer Perception and Sentiment Analysis:\nQuestion: How do sentiments expressed on social media and online forums regarding electric vehicles versus gasoline cars correlate with actual market trends? Can sentiment analysis predict shifts in consumer preferences?\n\n\nPredictive Maintenance for EVs:\nQuestion: Can we develop predictive maintenance models for electric vehicles based on data related to battery health, charging patterns, and driving conditions, to enhance the reliability and longevity of EVs?\n\n\nMarket Share Forecasting:\nQuestion: Using historical sales data, technological trends, and economic indicators, can we build a model to forecast the future market share of electric vehicles versus traditional cars, taking into account regional variations and global market dynamics?"
  }
]